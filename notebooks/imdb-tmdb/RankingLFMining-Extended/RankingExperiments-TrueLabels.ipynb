{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exempt-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "root_path = '../../../' # path to project root\n",
    "sys.path.append('{}/code'.format(root_path))\n",
    "sys.path.append('{}/code/core'.format(root_path))\n",
    "sys.path.append('{}/code/datasets/'.format(root_path))\n",
    "sys.path.insert(0,'{}/code/ptranking'.format(root_path))\n",
    "\n",
    "from core.ranking_utils import *\n",
    "from core.mallows import *\n",
    "from core.ws_ranking import *\n",
    "from core.ws_real_workflow import * \n",
    "from datasets.imdb_tmdb_dataset import * \n",
    "from datasets.basic_clmn_dataset import * \n",
    "from core.labelling.feature_lf import *\n",
    "from ptranking_wrapper import PtrankingWrapper\n",
    "import datasets_factory \n",
    "import numpy as np \n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sacred-column",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([50, 5, 16]) Y_train.shape torch.Size([50, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(50, 5, 16) (50, 5) (50,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-50', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-50', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [51.820713], train tau 0.2900000512599945, test_tau 0.3617999851703644,train_ndcg@1 tensor([0.7450]), test_ndcg@1 tensor([0.6750])\n",
      "epoch 1, loss [41.68579], train tau 0.22800004482269287, test_tau 0.34599995613098145,train_ndcg@1 tensor([0.8000]), test_ndcg@1 tensor([0.6730])\n",
      "epoch 2, loss [37.519775], train tau 0.20200008153915405, test_tau 0.3315999507904053,train_ndcg@1 tensor([0.8400]), test_ndcg@1 tensor([0.6850])\n",
      "epoch 3, loss [34.82945], train tau 0.17400005459785461, test_tau 0.33399999141693115,train_ndcg@1 tensor([0.8800]), test_ndcg@1 tensor([0.6805])\n",
      "epoch 4, loss [32.823063], train tau 0.1440000832080841, test_tau 0.3306000232696533,train_ndcg@1 tensor([0.9100]), test_ndcg@1 tensor([0.6855])\n",
      "epoch 5, loss [31.318512], train tau 0.12200009822845459, test_tau 0.32580000162124634,train_ndcg@1 tensor([0.9350]), test_ndcg@1 tensor([0.6850])\n",
      "epoch 6, loss [29.856115], train tau 0.11800003051757812, test_tau 0.326200008392334,train_ndcg@1 tensor([0.9400]), test_ndcg@1 tensor([0.6885])\n",
      "epoch 7, loss [28.783737], train tau 0.10200008749961853, test_tau 0.3232000470161438,train_ndcg@1 tensor([0.9400]), test_ndcg@1 tensor([0.6975])\n",
      "epoch 8, loss [27.281435], train tau 0.09400010108947754, test_tau 0.3232000172138214,train_ndcg@1 tensor([0.9450]), test_ndcg@1 tensor([0.6990])\n",
      "epoch 9, loss [26.091015], train tau 0.08000004291534424, test_tau 0.32340002059936523,train_ndcg@1 tensor([0.9700]), test_ndcg@1 tensor([0.7005])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-50/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([100, 5, 16]) Y_train.shape torch.Size([100, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(100, 5, 16) (100, 5) (100,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-100', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-100', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [92.81718], train tau 0.29100003838539124, test_tau 0.3418000340461731,train_ndcg@1 tensor([0.7450]), test_ndcg@1 tensor([0.6895])\n",
      "epoch 1, loss [82.16757], train tau 0.25599992275238037, test_tau 0.33660000562667847,train_ndcg@1 tensor([0.7850]), test_ndcg@1 tensor([0.6995])\n",
      "epoch 2, loss [78.96226], train tau 0.23999997973442078, test_tau 0.3357999920845032,train_ndcg@1 tensor([0.7925]), test_ndcg@1 tensor([0.6995])\n",
      "epoch 3, loss [75.715], train tau 0.22099992632865906, test_tau 0.3361999988555908,train_ndcg@1 tensor([0.8100]), test_ndcg@1 tensor([0.6990])\n",
      "epoch 4, loss [73.25193], train tau 0.21099993586540222, test_tau 0.3386000394821167,train_ndcg@1 tensor([0.8325]), test_ndcg@1 tensor([0.6975])\n",
      "epoch 5, loss [71.19554], train tau 0.21199986338615417, test_tau 0.339400053024292,train_ndcg@1 tensor([0.8225]), test_ndcg@1 tensor([0.6990])\n",
      "epoch 6, loss [70.169106], train tau 0.20299991965293884, test_tau 0.33400002121925354,train_ndcg@1 tensor([0.8350]), test_ndcg@1 tensor([0.6930])\n",
      "epoch 7, loss [68.19585], train tau 0.18999972939491272, test_tau 0.3349999785423279,train_ndcg@1 tensor([0.8550]), test_ndcg@1 tensor([0.7055])\n",
      "epoch 8, loss [67.25162], train tau 0.1739998161792755, test_tau 0.3306000232696533,train_ndcg@1 tensor([0.8400]), test_ndcg@1 tensor([0.7145])\n",
      "epoch 9, loss [65.61271], train tau 0.16899967193603516, test_tau 0.3277999758720398,train_ndcg@1 tensor([0.8400]), test_ndcg@1 tensor([0.7065])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-100/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([250, 5, 16]) Y_train.shape torch.Size([250, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(250, 5, 16) (250, 5) (250,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-250', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-250', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [234.2147], train tau 0.2935998737812042, test_tau 0.32499998807907104,train_ndcg@1 tensor([0.7040]), test_ndcg@1 tensor([0.7010])\n",
      "epoch 1, loss [206.59752], train tau 0.26799988746643066, test_tau 0.3179999589920044,train_ndcg@1 tensor([0.7480]), test_ndcg@1 tensor([0.7065])\n",
      "epoch 2, loss [198.29144], train tau 0.24519994854927063, test_tau 0.31679993867874146,train_ndcg@1 tensor([0.7730]), test_ndcg@1 tensor([0.7000])\n",
      "epoch 3, loss [192.96562], train tau 0.23799997568130493, test_tau 0.3137999176979065,train_ndcg@1 tensor([0.7860]), test_ndcg@1 tensor([0.7175])\n",
      "epoch 4, loss [188.52844], train tau 0.2255999743938446, test_tau 0.31579992175102234,train_ndcg@1 tensor([0.7910]), test_ndcg@1 tensor([0.7150])\n",
      "epoch 5, loss [184.27512], train tau 0.21479997038841248, test_tau 0.3157999515533447,train_ndcg@1 tensor([0.7990]), test_ndcg@1 tensor([0.7195])\n",
      "epoch 6, loss [180.42981], train tau 0.2071998119354248, test_tau 0.31800001859664917,train_ndcg@1 tensor([0.8090]), test_ndcg@1 tensor([0.7130])\n",
      "epoch 7, loss [178.41557], train tau 0.203599750995636, test_tau 0.32239997386932373,train_ndcg@1 tensor([0.8070]), test_ndcg@1 tensor([0.7080])\n",
      "epoch 8, loss [175.14406], train tau 0.1919998824596405, test_tau 0.32199999690055847,train_ndcg@1 tensor([0.8210]), test_ndcg@1 tensor([0.7040])\n",
      "epoch 9, loss [173.89273], train tau 0.20079991221427917, test_tau 0.3271999955177307,train_ndcg@1 tensor([0.8060]), test_ndcg@1 tensor([0.6960])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-250/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([500, 5, 16]) Y_train.shape torch.Size([500, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(500, 5, 16) (500, 5) (500,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-500', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-500', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [444.3724], train tau 0.2866002321243286, test_tau 0.3192000389099121,train_ndcg@1 tensor([0.7600]), test_ndcg@1 tensor([0.7295])\n",
      "epoch 1, loss [414.6182], train tau 0.2704002261161804, test_tau 0.312999963760376,train_ndcg@1 tensor([0.7640]), test_ndcg@1 tensor([0.7345])\n",
      "epoch 2, loss [401.79306], train tau 0.2580002248287201, test_tau 0.30779996514320374,train_ndcg@1 tensor([0.7810]), test_ndcg@1 tensor([0.7380])\n",
      "epoch 3, loss [393.45255], train tau 0.25020021200180054, test_tau 0.30639997124671936,train_ndcg@1 tensor([0.7895]), test_ndcg@1 tensor([0.7380])\n",
      "epoch 4, loss [386.44238], train tau 0.24140030145645142, test_tau 0.3051999807357788,train_ndcg@1 tensor([0.7965]), test_ndcg@1 tensor([0.7360])\n",
      "epoch 5, loss [379.81366], train tau 0.24140018224716187, test_tau 0.30699998140335083,train_ndcg@1 tensor([0.7995]), test_ndcg@1 tensor([0.7390])\n",
      "epoch 6, loss [374.70935], train tau 0.2312004268169403, test_tau 0.3107999861240387,train_ndcg@1 tensor([0.8075]), test_ndcg@1 tensor([0.7335])\n",
      "epoch 7, loss [369.55307], train tau 0.22280049324035645, test_tau 0.312000036239624,train_ndcg@1 tensor([0.8130]), test_ndcg@1 tensor([0.7360])\n",
      "epoch 8, loss [365.18182], train tau 0.21920064091682434, test_tau 0.3115999698638916,train_ndcg@1 tensor([0.8235]), test_ndcg@1 tensor([0.7340])\n",
      "epoch 9, loss [360.6712], train tau 0.21340063214302063, test_tau 0.30959999561309814,train_ndcg@1 tensor([0.8280]), test_ndcg@1 tensor([0.7360])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-500/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-1000', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-1000', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [865.04517], train tau 0.2821007966995239, test_tau 0.2943999767303467,train_ndcg@1 tensor([0.7387]), test_ndcg@1 tensor([0.7425])\n",
      "epoch 1, loss [813.5501], train tau 0.2683008909225464, test_tau 0.28700000047683716,train_ndcg@1 tensor([0.7592]), test_ndcg@1 tensor([0.7455])\n",
      "epoch 2, loss [795.57874], train tau 0.2565007209777832, test_tau 0.2871999740600586,train_ndcg@1 tensor([0.7805]), test_ndcg@1 tensor([0.7545])\n",
      "epoch 3, loss [785.0868], train tau 0.2469007670879364, test_tau 0.28679996728897095,train_ndcg@1 tensor([0.7890]), test_ndcg@1 tensor([0.7490])\n",
      "epoch 4, loss [777.77466], train tau 0.24330103397369385, test_tau 0.2853999733924866,train_ndcg@1 tensor([0.7915]), test_ndcg@1 tensor([0.7545])\n",
      "epoch 5, loss [771.1182], train tau 0.23910120129585266, test_tau 0.2892000079154968,train_ndcg@1 tensor([0.7922]), test_ndcg@1 tensor([0.7525])\n",
      "epoch 6, loss [766.1501], train tau 0.23870113492012024, test_tau 0.2842000126838684,train_ndcg@1 tensor([0.7970]), test_ndcg@1 tensor([0.7590])\n",
      "epoch 7, loss [761.7262], train tau 0.2339010238647461, test_tau 0.28620004653930664,train_ndcg@1 tensor([0.8008]), test_ndcg@1 tensor([0.7525])\n",
      "epoch 8, loss [757.8097], train tau 0.2326010763645172, test_tau 0.2850000262260437,train_ndcg@1 tensor([0.8027]), test_ndcg@1 tensor([0.7570])\n",
      "epoch 9, loss [752.5324], train tau 0.22700095176696777, test_tau 0.2868000268936157,train_ndcg@1 tensor([0.8055]), test_ndcg@1 tensor([0.7550])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-1000/result_summary.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([50, 5, 16]) Y_train.shape torch.Size([50, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(50, 5, 16) (50, 5) (50,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-50', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-50', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [51.614166], train tau 0.3059999942779541, test_tau 0.3977999985218048,train_ndcg@1 tensor([0.7200]), test_ndcg@1 tensor([0.6165])\n",
      "epoch 1, loss [42.170223], train tau 0.25000011920928955, test_tau 0.3758000135421753,train_ndcg@1 tensor([0.8200]), test_ndcg@1 tensor([0.6485])\n",
      "epoch 2, loss [38.934498], train tau 0.21000012755393982, test_tau 0.362000048160553,train_ndcg@1 tensor([0.8600]), test_ndcg@1 tensor([0.6625])\n",
      "epoch 3, loss [36.447136], train tau 0.19000014662742615, test_tau 0.3508000373840332,train_ndcg@1 tensor([0.8800]), test_ndcg@1 tensor([0.6790])\n",
      "epoch 4, loss [34.750206], train tau 0.16400012373924255, test_tau 0.3450000286102295,train_ndcg@1 tensor([0.9000]), test_ndcg@1 tensor([0.6910])\n",
      "epoch 5, loss [33.181164], train tau 0.15400013327598572, test_tau 0.34140002727508545,train_ndcg@1 tensor([0.9050]), test_ndcg@1 tensor([0.7025])\n",
      "epoch 6, loss [31.86776], train tau 0.14400005340576172, test_tau 0.34140002727508545,train_ndcg@1 tensor([0.9200]), test_ndcg@1 tensor([0.7025])\n",
      "epoch 7, loss [30.54625], train tau 0.14000004529953003, test_tau 0.34219998121261597,train_ndcg@1 tensor([0.9250]), test_ndcg@1 tensor([0.6995])\n",
      "epoch 8, loss [29.125078], train tau 0.12200000882148743, test_tau 0.3407999873161316,train_ndcg@1 tensor([0.9100]), test_ndcg@1 tensor([0.7100])\n",
      "epoch 9, loss [27.93124], train tau 0.11799991130828857, test_tau 0.3402000069618225,train_ndcg@1 tensor([0.9250]), test_ndcg@1 tensor([0.7090])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-50/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([100, 5, 16]) Y_train.shape torch.Size([100, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(100, 5, 16) (100, 5) (100,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-100', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-100', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [93.24496], train tau 0.30299997329711914, test_tau 0.343999981880188,train_ndcg@1 tensor([0.7275]), test_ndcg@1 tensor([0.6800])\n",
      "epoch 1, loss [84.139084], train tau 0.26600003242492676, test_tau 0.33820000290870667,train_ndcg@1 tensor([0.7550]), test_ndcg@1 tensor([0.6900])\n",
      "epoch 2, loss [79.60105], train tau 0.2510000467300415, test_tau 0.3319999873638153,train_ndcg@1 tensor([0.7750]), test_ndcg@1 tensor([0.6930])\n",
      "epoch 3, loss [77.24593], train tau 0.22700008749961853, test_tau 0.32999998331069946,train_ndcg@1 tensor([0.7900]), test_ndcg@1 tensor([0.7035])\n",
      "epoch 4, loss [74.54951], train tau 0.21700000762939453, test_tau 0.3302000164985657,train_ndcg@1 tensor([0.8075]), test_ndcg@1 tensor([0.6980])\n",
      "epoch 5, loss [72.22174], train tau 0.19899991154670715, test_tau 0.3306000232696533,train_ndcg@1 tensor([0.8200]), test_ndcg@1 tensor([0.7010])\n",
      "epoch 6, loss [70.924], train tau 0.19299989938735962, test_tau 0.33379998803138733,train_ndcg@1 tensor([0.8300]), test_ndcg@1 tensor([0.6815])\n",
      "epoch 7, loss [68.88713], train tau 0.18499982357025146, test_tau 0.33559998869895935,train_ndcg@1 tensor([0.8300]), test_ndcg@1 tensor([0.6810])\n",
      "epoch 8, loss [67.15064], train tau 0.17499980330467224, test_tau 0.33260002732276917,train_ndcg@1 tensor([0.8400]), test_ndcg@1 tensor([0.6865])\n",
      "epoch 9, loss [65.38285], train tau 0.17099988460540771, test_tau 0.33219999074935913,train_ndcg@1 tensor([0.8475]), test_ndcg@1 tensor([0.6895])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-100/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([250, 5, 16]) Y_train.shape torch.Size([250, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(250, 5, 16) (250, 5) (250,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-250', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-250', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [239.20232], train tau 0.30879974365234375, test_tau 0.34460002183914185,train_ndcg@1 tensor([0.7190]), test_ndcg@1 tensor([0.6835])\n",
      "epoch 1, loss [215.94139], train tau 0.2879996597766876, test_tau 0.3296000063419342,train_ndcg@1 tensor([0.7450]), test_ndcg@1 tensor([0.6910])\n",
      "epoch 2, loss [208.0192], train tau 0.2723998725414276, test_tau 0.3198000192642212,train_ndcg@1 tensor([0.7570]), test_ndcg@1 tensor([0.6925])\n",
      "epoch 3, loss [202.81322], train tau 0.25639986991882324, test_tau 0.3181999921798706,train_ndcg@1 tensor([0.7840]), test_ndcg@1 tensor([0.6985])\n",
      "epoch 4, loss [198.8264], train tau 0.24799972772598267, test_tau 0.31539997458457947,train_ndcg@1 tensor([0.7930]), test_ndcg@1 tensor([0.7005])\n",
      "epoch 5, loss [195.6335], train tau 0.24159973859786987, test_tau 0.31519997119903564,train_ndcg@1 tensor([0.8000]), test_ndcg@1 tensor([0.6970])\n",
      "epoch 6, loss [192.72961], train tau 0.2343997359275818, test_tau 0.3179999887943268,train_ndcg@1 tensor([0.8010]), test_ndcg@1 tensor([0.6910])\n",
      "epoch 7, loss [189.19821], train tau 0.2339998483657837, test_tau 0.3181999921798706,train_ndcg@1 tensor([0.8050]), test_ndcg@1 tensor([0.6810])\n",
      "epoch 8, loss [186.46898], train tau 0.22519993782043457, test_tau 0.3172000050544739,train_ndcg@1 tensor([0.8150]), test_ndcg@1 tensor([0.6860])\n",
      "epoch 9, loss [183.13142], train tau 0.21999996900558472, test_tau 0.3215999901294708,train_ndcg@1 tensor([0.8250]), test_ndcg@1 tensor([0.6840])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-250/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([500, 5, 16]) Y_train.shape torch.Size([500, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(500, 5, 16) (500, 5) (500,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-500', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-500', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [440.88568], train tau 0.27980002760887146, test_tau 0.3200000524520874,train_ndcg@1 tensor([0.7645]), test_ndcg@1 tensor([0.7180])\n",
      "epoch 1, loss [409.46515], train tau 0.2538001835346222, test_tau 0.31320005655288696,train_ndcg@1 tensor([0.7940]), test_ndcg@1 tensor([0.7220])\n",
      "epoch 2, loss [398.30606], train tau 0.24420017004013062, test_tau 0.3077999949455261,train_ndcg@1 tensor([0.8090]), test_ndcg@1 tensor([0.7310])\n",
      "epoch 3, loss [388.634], train tau 0.23660030961036682, test_tau 0.3019999861717224,train_ndcg@1 tensor([0.8075]), test_ndcg@1 tensor([0.7285])\n",
      "epoch 4, loss [381.26657], train tau 0.23040050268173218, test_tau 0.30159997940063477,train_ndcg@1 tensor([0.8060]), test_ndcg@1 tensor([0.7280])\n",
      "epoch 5, loss [374.47394], train tau 0.21900060772895813, test_tau 0.30300000309944153,train_ndcg@1 tensor([0.8245]), test_ndcg@1 tensor([0.7250])\n",
      "epoch 6, loss [369.50427], train tau 0.21360060572624207, test_tau 0.3035999536514282,train_ndcg@1 tensor([0.8315]), test_ndcg@1 tensor([0.7230])\n",
      "epoch 7, loss [363.9903], train tau 0.21060079336166382, test_tau 0.30459994077682495,train_ndcg@1 tensor([0.8340]), test_ndcg@1 tensor([0.7175])\n",
      "epoch 8, loss [358.39], train tau 0.20220071077346802, test_tau 0.3012000024318695,train_ndcg@1 tensor([0.8430]), test_ndcg@1 tensor([0.7225])\n",
      "epoch 9, loss [353.89145], train tau 0.2002006471157074, test_tau 0.3025999963283539,train_ndcg@1 tensor([0.8425]), test_ndcg@1 tensor([0.7155])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-500/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-1000', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-1000', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [869.4163], train tau 0.2885003685951233, test_tau 0.29360002279281616,train_ndcg@1 tensor([0.7442]), test_ndcg@1 tensor([0.7355])\n",
      "epoch 1, loss [820.43335], train tau 0.2741006016731262, test_tau 0.2810000777244568,train_ndcg@1 tensor([0.7707]), test_ndcg@1 tensor([0.7495])\n",
      "epoch 2, loss [804.8938], train tau 0.2646006941795349, test_tau 0.28220003843307495,train_ndcg@1 tensor([0.7843]), test_ndcg@1 tensor([0.7530])\n",
      "epoch 3, loss [792.62604], train tau 0.2602007985115051, test_tau 0.28040003776550293,train_ndcg@1 tensor([0.7872]), test_ndcg@1 tensor([0.7580])\n",
      "epoch 4, loss [782.7864], train tau 0.25200098752975464, test_tau 0.2778000235557556,train_ndcg@1 tensor([0.7900]), test_ndcg@1 tensor([0.7625])\n",
      "epoch 5, loss [774.7354], train tau 0.24760094285011292, test_tau 0.2842000126838684,train_ndcg@1 tensor([0.7960]), test_ndcg@1 tensor([0.7580])\n",
      "epoch 6, loss [770.0468], train tau 0.24720090627670288, test_tau 0.28700003027915955,train_ndcg@1 tensor([0.7950]), test_ndcg@1 tensor([0.7555])\n",
      "epoch 7, loss [764.6918], train tau 0.24500086903572083, test_tau 0.2896000146865845,train_ndcg@1 tensor([0.7985]), test_ndcg@1 tensor([0.7470])\n",
      "epoch 8, loss [757.60565], train tau 0.24150121212005615, test_tau 0.2922000288963318,train_ndcg@1 tensor([0.8012]), test_ndcg@1 tensor([0.7485])\n",
      "epoch 9, loss [754.0606], train tau 0.24090096354484558, test_tau 0.28600001335144043,train_ndcg@1 tensor([0.8030]), test_ndcg@1 tensor([0.7440])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-1000/result_summary.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([50, 5, 16]) Y_train.shape torch.Size([50, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(50, 5, 16) (50, 5) (50,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-50', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-50', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [47.11392], train tau 0.2940000891685486, test_tau 0.3569999635219574,train_ndcg@1 tensor([0.7400]), test_ndcg@1 tensor([0.6545])\n",
      "epoch 1, loss [40.87586], train tau 0.23600003123283386, test_tau 0.3452000021934509,train_ndcg@1 tensor([0.8100]), test_ndcg@1 tensor([0.6620])\n",
      "epoch 2, loss [38.32945], train tau 0.2080000340938568, test_tau 0.33779996633529663,train_ndcg@1 tensor([0.8300]), test_ndcg@1 tensor([0.6790])\n",
      "epoch 3, loss [36.297447], train tau 0.2040000855922699, test_tau 0.33319997787475586,train_ndcg@1 tensor([0.8150]), test_ndcg@1 tensor([0.6925])\n",
      "epoch 4, loss [34.764267], train tau 0.19000011682510376, test_tau 0.33459997177124023,train_ndcg@1 tensor([0.8550]), test_ndcg@1 tensor([0.6780])\n",
      "epoch 5, loss [33.395462], train tau 0.18400010466575623, test_tau 0.33779996633529663,train_ndcg@1 tensor([0.8700]), test_ndcg@1 tensor([0.6755])\n",
      "epoch 6, loss [32.186497], train tau 0.17200011014938354, test_tau 0.3375999927520752,train_ndcg@1 tensor([0.8750]), test_ndcg@1 tensor([0.6810])\n",
      "epoch 7, loss [31.080986], train tau 0.166000097990036, test_tau 0.33559995889663696,train_ndcg@1 tensor([0.8800]), test_ndcg@1 tensor([0.6795])\n",
      "epoch 8, loss [30.108734], train tau 0.1560000479221344, test_tau 0.3335999846458435,train_ndcg@1 tensor([0.8900]), test_ndcg@1 tensor([0.6800])\n",
      "epoch 9, loss [28.995523], train tau 0.14399996399879456, test_tau 0.3330000042915344,train_ndcg@1 tensor([0.8900]), test_ndcg@1 tensor([0.6850])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-50/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([100, 5, 16]) Y_train.shape torch.Size([100, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(100, 5, 16) (100, 5) (100,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-100', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-100', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [101.11774], train tau 0.3050000071525574, test_tau 0.3747999966144562,train_ndcg@1 tensor([0.7325]), test_ndcg@1 tensor([0.6265])\n",
      "epoch 1, loss [86.91885], train tau 0.2820000648498535, test_tau 0.3610000014305115,train_ndcg@1 tensor([0.7725]), test_ndcg@1 tensor([0.6525])\n",
      "epoch 2, loss [82.65813], train tau 0.2590000033378601, test_tau 0.35020002722740173,train_ndcg@1 tensor([0.7825]), test_ndcg@1 tensor([0.6705])\n",
      "epoch 3, loss [79.71082], train tau 0.24700003862380981, test_tau 0.3416000008583069,train_ndcg@1 tensor([0.7975]), test_ndcg@1 tensor([0.6750])\n",
      "epoch 4, loss [77.14186], train tau 0.23500001430511475, test_tau 0.3402000069618225,train_ndcg@1 tensor([0.8125]), test_ndcg@1 tensor([0.6755])\n",
      "epoch 5, loss [75.22596], train tau 0.21599990129470825, test_tau 0.3433999717235565,train_ndcg@1 tensor([0.8125]), test_ndcg@1 tensor([0.6810])\n",
      "epoch 6, loss [73.24171], train tau 0.20299991965293884, test_tau 0.340999960899353,train_ndcg@1 tensor([0.8125]), test_ndcg@1 tensor([0.6840])\n",
      "epoch 7, loss [71.23345], train tau 0.19399979710578918, test_tau 0.34379997849464417,train_ndcg@1 tensor([0.8325]), test_ndcg@1 tensor([0.6825])\n",
      "epoch 8, loss [69.26355], train tau 0.1809997856616974, test_tau 0.3442000150680542,train_ndcg@1 tensor([0.8550]), test_ndcg@1 tensor([0.6745])\n",
      "epoch 9, loss [67.734856], train tau 0.1699998378753662, test_tau 0.34880000352859497,train_ndcg@1 tensor([0.8675]), test_ndcg@1 tensor([0.6655])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-100/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([250, 5, 16]) Y_train.shape torch.Size([250, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(250, 5, 16) (250, 5) (250,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-250', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-250', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [229.84734], train tau 0.3003997802734375, test_tau 0.32159996032714844,train_ndcg@1 tensor([0.7310]), test_ndcg@1 tensor([0.7030])\n",
      "epoch 1, loss [208.65373], train tau 0.275199830532074, test_tau 0.3075999915599823,train_ndcg@1 tensor([0.7750]), test_ndcg@1 tensor([0.7205])\n",
      "epoch 2, loss [202.07071], train tau 0.26439976692199707, test_tau 0.3051999807357788,train_ndcg@1 tensor([0.7850]), test_ndcg@1 tensor([0.7340])\n",
      "epoch 3, loss [197.14877], train tau 0.2527996897697449, test_tau 0.29899996519088745,train_ndcg@1 tensor([0.7880]), test_ndcg@1 tensor([0.7380])\n",
      "epoch 4, loss [192.47894], train tau 0.24599972367286682, test_tau 0.2953999638557434,train_ndcg@1 tensor([0.7870]), test_ndcg@1 tensor([0.7360])\n",
      "epoch 5, loss [189.12373], train tau 0.23959973454475403, test_tau 0.29319998621940613,train_ndcg@1 tensor([0.7880]), test_ndcg@1 tensor([0.7425])\n",
      "epoch 6, loss [185.15257], train tau 0.23199987411499023, test_tau 0.2979999780654907,train_ndcg@1 tensor([0.8040]), test_ndcg@1 tensor([0.7395])\n",
      "epoch 7, loss [182.92436], train tau 0.22559988498687744, test_tau 0.2977999448776245,train_ndcg@1 tensor([0.8070]), test_ndcg@1 tensor([0.7350])\n",
      "epoch 8, loss [179.78458], train tau 0.21519982814788818, test_tau 0.3025999665260315,train_ndcg@1 tensor([0.8340]), test_ndcg@1 tensor([0.7380])\n",
      "epoch 9, loss [177.43556], train tau 0.2131999135017395, test_tau 0.3023999333381653,train_ndcg@1 tensor([0.8250]), test_ndcg@1 tensor([0.7360])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-250/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([500, 5, 16]) Y_train.shape torch.Size([500, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(500, 5, 16) (500, 5) (500,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-500', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-500', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [432.85333], train tau 0.27180016040802, test_tau 0.2985999584197998,train_ndcg@1 tensor([0.7720]), test_ndcg@1 tensor([0.7440])\n",
      "epoch 1, loss [405.55377], train tau 0.2578003406524658, test_tau 0.29099997878074646,train_ndcg@1 tensor([0.7840]), test_ndcg@1 tensor([0.7550])\n",
      "epoch 2, loss [396.24838], train tau 0.24940034747123718, test_tau 0.2829999625682831,train_ndcg@1 tensor([0.7910]), test_ndcg@1 tensor([0.7520])\n",
      "epoch 3, loss [390.94345], train tau 0.24880027770996094, test_tau 0.2849999666213989,train_ndcg@1 tensor([0.7915]), test_ndcg@1 tensor([0.7500])\n",
      "epoch 4, loss [386.95642], train tau 0.241000235080719, test_tau 0.28519999980926514,train_ndcg@1 tensor([0.8065]), test_ndcg@1 tensor([0.7515])\n",
      "epoch 5, loss [381.86505], train tau 0.2308003306388855, test_tau 0.2837999761104584,train_ndcg@1 tensor([0.8145]), test_ndcg@1 tensor([0.7510])\n",
      "epoch 6, loss [376.8945], train tau 0.2278004288673401, test_tau 0.2829999327659607,train_ndcg@1 tensor([0.8160]), test_ndcg@1 tensor([0.7615])\n",
      "epoch 7, loss [372.41663], train tau 0.21840035915374756, test_tau 0.28439998626708984,train_ndcg@1 tensor([0.8260]), test_ndcg@1 tensor([0.7520])\n",
      "epoch 8, loss [367.5961], train tau 0.22000056505203247, test_tau 0.2845999598503113,train_ndcg@1 tensor([0.8260]), test_ndcg@1 tensor([0.7500])\n",
      "epoch 9, loss [363.1714], train tau 0.21580052375793457, test_tau 0.28700000047683716,train_ndcg@1 tensor([0.8240]), test_ndcg@1 tensor([0.7540])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-500/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-1000', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-1000', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [867.38336], train tau 0.27900052070617676, test_tau 0.2922000288963318,train_ndcg@1 tensor([0.7542]), test_ndcg@1 tensor([0.7365])\n",
      "epoch 1, loss [812.74475], train tau 0.269500732421875, test_tau 0.28860002756118774,train_ndcg@1 tensor([0.7685]), test_ndcg@1 tensor([0.7410])\n",
      "epoch 2, loss [798.26166], train tau 0.2617012560367584, test_tau 0.28720003366470337,train_ndcg@1 tensor([0.7757]), test_ndcg@1 tensor([0.7310])\n",
      "epoch 3, loss [788.2944], train tau 0.2557011842727661, test_tau 0.29040005803108215,train_ndcg@1 tensor([0.7822]), test_ndcg@1 tensor([0.7360])\n",
      "epoch 4, loss [783.84106], train tau 0.25290125608444214, test_tau 0.2919999957084656,train_ndcg@1 tensor([0.7922]), test_ndcg@1 tensor([0.7360])\n",
      "epoch 5, loss [776.28644], train tau 0.2493014931678772, test_tau 0.295600026845932,train_ndcg@1 tensor([0.7933]), test_ndcg@1 tensor([0.7365])\n",
      "epoch 6, loss [770.0845], train tau 0.24670138955116272, test_tau 0.2924000024795532,train_ndcg@1 tensor([0.7977]), test_ndcg@1 tensor([0.7360])\n",
      "epoch 7, loss [764.7602], train tau 0.24250131845474243, test_tau 0.2888000011444092,train_ndcg@1 tensor([0.8035]), test_ndcg@1 tensor([0.7455])\n",
      "epoch 8, loss [757.13574], train tau 0.24030110239982605, test_tau 0.28380003571510315,train_ndcg@1 tensor([0.8000]), test_ndcg@1 tensor([0.7560])\n",
      "epoch 9, loss [751.98035], train tau 0.23710092902183533, test_tau 0.2888000011444092,train_ndcg@1 tensor([0.8010]), test_ndcg@1 tensor([0.7440])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-1000/result_summary.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([50, 5, 16]) Y_train.shape torch.Size([50, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(50, 5, 16) (50, 5) (50,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-50', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-50', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [49.00836], train tau 0.2800000011920929, test_tau 0.3752000033855438,train_ndcg@1 tensor([0.7600]), test_ndcg@1 tensor([0.6155])\n",
      "epoch 1, loss [39.524353], train tau 0.22000014781951904, test_tau 0.3612000048160553,train_ndcg@1 tensor([0.8350]), test_ndcg@1 tensor([0.6340])\n",
      "epoch 2, loss [36.81564], train tau 0.1880001425743103, test_tau 0.35180002450942993,train_ndcg@1 tensor([0.8550]), test_ndcg@1 tensor([0.6535])\n",
      "epoch 3, loss [34.812115], train tau 0.17400014400482178, test_tau 0.3456000089645386,train_ndcg@1 tensor([0.8600]), test_ndcg@1 tensor([0.6610])\n",
      "epoch 4, loss [32.792465], train tau 0.17200008034706116, test_tau 0.3458000421524048,train_ndcg@1 tensor([0.8800]), test_ndcg@1 tensor([0.6650])\n",
      "epoch 5, loss [31.113022], train tau 0.16399997472763062, test_tau 0.3410000205039978,train_ndcg@1 tensor([0.8800]), test_ndcg@1 tensor([0.6710])\n",
      "epoch 6, loss [29.756596], train tau 0.15000000596046448, test_tau 0.3425999879837036,train_ndcg@1 tensor([0.8850]), test_ndcg@1 tensor([0.6685])\n",
      "epoch 7, loss [28.608913], train tau 0.13600006699562073, test_tau 0.344400018453598,train_ndcg@1 tensor([0.9000]), test_ndcg@1 tensor([0.6755])\n",
      "epoch 8, loss [27.588205], train tau 0.1260000467300415, test_tau 0.3428000211715698,train_ndcg@1 tensor([0.9100]), test_ndcg@1 tensor([0.6750])\n",
      "epoch 9, loss [27.27918], train tau 0.13199999928474426, test_tau 0.3468000292778015,train_ndcg@1 tensor([0.9150]), test_ndcg@1 tensor([0.6735])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-50/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([100, 5, 16]) Y_train.shape torch.Size([100, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(100, 5, 16) (100, 5) (100,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-100', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-100', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [97.771255], train tau 0.30400002002716064, test_tau 0.35899999737739563,train_ndcg@1 tensor([0.7125]), test_ndcg@1 tensor([0.6390])\n",
      "epoch 1, loss [84.63693], train tau 0.27300000190734863, test_tau 0.34539997577667236,train_ndcg@1 tensor([0.7575]), test_ndcg@1 tensor([0.6620])\n",
      "epoch 2, loss [79.753944], train tau 0.2510001063346863, test_tau 0.33560001850128174,train_ndcg@1 tensor([0.7875]), test_ndcg@1 tensor([0.6775])\n",
      "epoch 3, loss [77.0313], train tau 0.2250000238418579, test_tau 0.3328000009059906,train_ndcg@1 tensor([0.8425]), test_ndcg@1 tensor([0.6785])\n",
      "epoch 4, loss [74.85153], train tau 0.22199994325637817, test_tau 0.32780003547668457,train_ndcg@1 tensor([0.8425]), test_ndcg@1 tensor([0.6830])\n",
      "epoch 5, loss [72.969444], train tau 0.21299991011619568, test_tau 0.3290000259876251,train_ndcg@1 tensor([0.8575]), test_ndcg@1 tensor([0.6905])\n",
      "epoch 6, loss [71.66292], train tau 0.20499998331069946, test_tau 0.3240000605583191,train_ndcg@1 tensor([0.8650]), test_ndcg@1 tensor([0.6980])\n",
      "epoch 7, loss [69.72216], train tau 0.19799989461898804, test_tau 0.32580000162124634,train_ndcg@1 tensor([0.8725]), test_ndcg@1 tensor([0.6965])\n",
      "epoch 8, loss [68.26574], train tau 0.182999849319458, test_tau 0.3270000219345093,train_ndcg@1 tensor([0.8925]), test_ndcg@1 tensor([0.6990])\n",
      "epoch 9, loss [66.3279], train tau 0.17299982905387878, test_tau 0.329800009727478,train_ndcg@1 tensor([0.8875]), test_ndcg@1 tensor([0.6920])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-100/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([250, 5, 16]) Y_train.shape torch.Size([250, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(250, 5, 16) (250, 5) (250,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-250', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-250', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [232.90149], train tau 0.2911997437477112, test_tau 0.3253999948501587,train_ndcg@1 tensor([0.7520]), test_ndcg@1 tensor([0.6985])\n",
      "epoch 1, loss [207.65271], train tau 0.2635996341705322, test_tau 0.3140000104904175,train_ndcg@1 tensor([0.7750]), test_ndcg@1 tensor([0.7070])\n",
      "epoch 2, loss [200.29626], train tau 0.250399649143219, test_tau 0.30799999833106995,train_ndcg@1 tensor([0.7970]), test_ndcg@1 tensor([0.7105])\n",
      "epoch 3, loss [195.0483], train tau 0.2427997887134552, test_tau 0.31139999628067017,train_ndcg@1 tensor([0.8010]), test_ndcg@1 tensor([0.7050])\n",
      "epoch 4, loss [190.581], train tau 0.2355998158454895, test_tau 0.30559998750686646,train_ndcg@1 tensor([0.8140]), test_ndcg@1 tensor([0.7130])\n",
      "epoch 5, loss [187.53851], train tau 0.22479978203773499, test_tau 0.3051999807357788,train_ndcg@1 tensor([0.8180]), test_ndcg@1 tensor([0.7220])\n",
      "epoch 6, loss [183.85648], train tau 0.21399983763694763, test_tau 0.30379998683929443,train_ndcg@1 tensor([0.8310]), test_ndcg@1 tensor([0.7270])\n",
      "epoch 7, loss [180.25601], train tau 0.20359981060028076, test_tau 0.30820000171661377,train_ndcg@1 tensor([0.8400]), test_ndcg@1 tensor([0.7220])\n",
      "epoch 8, loss [176.69484], train tau 0.19719991087913513, test_tau 0.30820000171661377,train_ndcg@1 tensor([0.8410]), test_ndcg@1 tensor([0.7280])\n",
      "epoch 9, loss [173.18826], train tau 0.20079991221427917, test_tau 0.31139999628067017,train_ndcg@1 tensor([0.8420]), test_ndcg@1 tensor([0.7215])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-250/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([500, 5, 16]) Y_train.shape torch.Size([500, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(500, 5, 16) (500, 5) (500,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-500', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-500', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [439.72913], train tau 0.276800274848938, test_tau 0.3107999563217163,train_ndcg@1 tensor([0.7610]), test_ndcg@1 tensor([0.7315])\n",
      "epoch 1, loss [407.7058], train tau 0.2638005018234253, test_tau 0.30140000581741333,train_ndcg@1 tensor([0.7795]), test_ndcg@1 tensor([0.7405])\n",
      "epoch 2, loss [396.82233], train tau 0.2560003995895386, test_tau 0.30139994621276855,train_ndcg@1 tensor([0.7860]), test_ndcg@1 tensor([0.7405])\n",
      "epoch 3, loss [389.19662], train tau 0.24860021471977234, test_tau 0.3027999699115753,train_ndcg@1 tensor([0.8015]), test_ndcg@1 tensor([0.7380])\n",
      "epoch 4, loss [382.49765], train tau 0.2414003610610962, test_tau 0.29899996519088745,train_ndcg@1 tensor([0.8000]), test_ndcg@1 tensor([0.7405])\n",
      "epoch 5, loss [377.2951], train tau 0.23420032858848572, test_tau 0.3017999529838562,train_ndcg@1 tensor([0.8030]), test_ndcg@1 tensor([0.7395])\n",
      "epoch 6, loss [372.09183], train tau 0.23200026154518127, test_tau 0.3001999855041504,train_ndcg@1 tensor([0.8055]), test_ndcg@1 tensor([0.7405])\n",
      "epoch 7, loss [367.73074], train tau 0.22440028190612793, test_tau 0.3012000024318695,train_ndcg@1 tensor([0.8130]), test_ndcg@1 tensor([0.7425])\n",
      "epoch 8, loss [364.27475], train tau 0.22140035033226013, test_tau 0.3020000159740448,train_ndcg@1 tensor([0.8150]), test_ndcg@1 tensor([0.7465])\n",
      "epoch 9, loss [360.82977], train tau 0.21840018033981323, test_tau 0.3046000003814697,train_ndcg@1 tensor([0.8160]), test_ndcg@1 tensor([0.7420])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-500/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-1000', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-1000', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [851.07074], train tau 0.27050045132637024, test_tau 0.2897999882698059,train_ndcg@1 tensor([0.7650]), test_ndcg@1 tensor([0.7390])\n",
      "epoch 1, loss [806.4785], train tau 0.2591007947921753, test_tau 0.2842000126838684,train_ndcg@1 tensor([0.7760]), test_ndcg@1 tensor([0.7445])\n",
      "epoch 2, loss [794.10443], train tau 0.2520008683204651, test_tau 0.2826000154018402,train_ndcg@1 tensor([0.7912]), test_ndcg@1 tensor([0.7410])\n",
      "epoch 3, loss [785.08325], train tau 0.2463010549545288, test_tau 0.28519999980926514,train_ndcg@1 tensor([0.7937]), test_ndcg@1 tensor([0.7515])\n",
      "epoch 4, loss [777.08527], train tau 0.24320116639137268, test_tau 0.2833999991416931,train_ndcg@1 tensor([0.8000]), test_ndcg@1 tensor([0.7530])\n",
      "epoch 5, loss [770.3959], train tau 0.23800069093704224, test_tau 0.2881999909877777,train_ndcg@1 tensor([0.8065]), test_ndcg@1 tensor([0.7420])\n",
      "epoch 6, loss [765.1489], train tau 0.2387009561061859, test_tau 0.2879999876022339,train_ndcg@1 tensor([0.8062]), test_ndcg@1 tensor([0.7340])\n",
      "epoch 7, loss [758.9717], train tau 0.23340114951133728, test_tau 0.2900000214576721,train_ndcg@1 tensor([0.8067]), test_ndcg@1 tensor([0.7345])\n",
      "epoch 8, loss [752.8158], train tau 0.22810080647468567, test_tau 0.290800005197525,train_ndcg@1 tensor([0.8140]), test_ndcg@1 tensor([0.7315])\n",
      "epoch 9, loss [748.3057], train tau 0.22720077633857727, test_tau 0.2986000180244446,train_ndcg@1 tensor([0.8177]), test_ndcg@1 tensor([0.7225])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-1000/result_summary.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([50, 5, 16]) Y_train.shape torch.Size([50, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(50, 5, 16) (50, 5) (50,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-50', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-50', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [52.96413], train tau 0.28999996185302734, test_tau 0.39180001616477966,train_ndcg@1 tensor([0.7450]), test_ndcg@1 tensor([0.6450])\n",
      "epoch 1, loss [40.440025], train tau 0.22600007057189941, test_tau 0.3677999973297119,train_ndcg@1 tensor([0.8250]), test_ndcg@1 tensor([0.6680])\n",
      "epoch 2, loss [36.57857], train tau 0.19600018858909607, test_tau 0.3540000319480896,train_ndcg@1 tensor([0.8550]), test_ndcg@1 tensor([0.6790])\n",
      "epoch 3, loss [34.255344], train tau 0.18200013041496277, test_tau 0.350600004196167,train_ndcg@1 tensor([0.8850]), test_ndcg@1 tensor([0.6850])\n",
      "epoch 4, loss [32.470337], train tau 0.15600010752677917, test_tau 0.3508000373840332,train_ndcg@1 tensor([0.9100]), test_ndcg@1 tensor([0.6845])\n",
      "epoch 5, loss [30.964136], train tau 0.13400009274482727, test_tau 0.35180002450942993,train_ndcg@1 tensor([0.9200]), test_ndcg@1 tensor([0.6735])\n",
      "epoch 6, loss [29.892332], train tau 0.1320001184940338, test_tau 0.3508000373840332,train_ndcg@1 tensor([0.9200]), test_ndcg@1 tensor([0.6715])\n",
      "epoch 7, loss [28.656742], train tau 0.12800008058547974, test_tau 0.35200002789497375,train_ndcg@1 tensor([0.9100]), test_ndcg@1 tensor([0.6700])\n",
      "epoch 8, loss [27.572607], train tau 0.11200007796287537, test_tau 0.35300004482269287,train_ndcg@1 tensor([0.9250]), test_ndcg@1 tensor([0.6735])\n",
      "epoch 9, loss [26.481453], train tau 0.10600003600120544, test_tau 0.35199999809265137,train_ndcg@1 tensor([0.9300]), test_ndcg@1 tensor([0.6740])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-50/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([100, 5, 16]) Y_train.shape torch.Size([100, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(100, 5, 16) (100, 5) (100,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-100', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-100', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [96.58134], train tau 0.3190000355243683, test_tau 0.35760003328323364,train_ndcg@1 tensor([0.7050]), test_ndcg@1 tensor([0.6835])\n",
      "epoch 1, loss [85.780205], train tau 0.27300000190734863, test_tau 0.34200000762939453,train_ndcg@1 tensor([0.7650]), test_ndcg@1 tensor([0.6890])\n",
      "epoch 2, loss [81.759445], train tau 0.247999906539917, test_tau 0.3357999920845032,train_ndcg@1 tensor([0.8075]), test_ndcg@1 tensor([0.6920])\n",
      "epoch 3, loss [79.456985], train tau 0.2359999418258667, test_tau 0.3322000205516815,train_ndcg@1 tensor([0.8125]), test_ndcg@1 tensor([0.6960])\n",
      "epoch 4, loss [77.92816], train tau 0.22999989986419678, test_tau 0.33219999074935913,train_ndcg@1 tensor([0.8250]), test_ndcg@1 tensor([0.6990])\n",
      "epoch 5, loss [76.24022], train tau 0.21599987149238586, test_tau 0.33160001039505005,train_ndcg@1 tensor([0.8425]), test_ndcg@1 tensor([0.6880])\n",
      "epoch 6, loss [74.725586], train tau 0.21599990129470825, test_tau 0.33639997243881226,train_ndcg@1 tensor([0.8375]), test_ndcg@1 tensor([0.6900])\n",
      "epoch 7, loss [73.29119], train tau 0.20999985933303833, test_tau 0.335999995470047,train_ndcg@1 tensor([0.8450]), test_ndcg@1 tensor([0.6870])\n",
      "epoch 8, loss [71.69484], train tau 0.20599988102912903, test_tau 0.3381999731063843,train_ndcg@1 tensor([0.8550]), test_ndcg@1 tensor([0.6810])\n",
      "epoch 9, loss [70.44836], train tau 0.19699981808662415, test_tau 0.34039998054504395,train_ndcg@1 tensor([0.8625]), test_ndcg@1 tensor([0.6810])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-100/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([250, 5, 16]) Y_train.shape torch.Size([250, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(250, 5, 16) (250, 5) (250,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-250', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-250', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [236.62682], train tau 0.3096001148223877, test_tau 0.34199994802474976,train_ndcg@1 tensor([0.7140]), test_ndcg@1 tensor([0.6895])\n",
      "epoch 1, loss [210.63301], train tau 0.29279983043670654, test_tau 0.3263999819755554,train_ndcg@1 tensor([0.7400]), test_ndcg@1 tensor([0.6960])\n",
      "epoch 2, loss [203.87129], train tau 0.27439987659454346, test_tau 0.3157999515533447,train_ndcg@1 tensor([0.7590]), test_ndcg@1 tensor([0.7025])\n",
      "epoch 3, loss [199.4181], train tau 0.2655998468399048, test_tau 0.31199994683265686,train_ndcg@1 tensor([0.7700]), test_ndcg@1 tensor([0.7085])\n",
      "epoch 4, loss [195.81026], train tau 0.24759960174560547, test_tau 0.30699998140335083,train_ndcg@1 tensor([0.7950]), test_ndcg@1 tensor([0.7105])\n",
      "epoch 5, loss [192.17244], train tau 0.24759963154792786, test_tau 0.30699998140335083,train_ndcg@1 tensor([0.7930]), test_ndcg@1 tensor([0.7175])\n",
      "epoch 6, loss [189.20985], train tau 0.2315996289253235, test_tau 0.3019999861717224,train_ndcg@1 tensor([0.8060]), test_ndcg@1 tensor([0.7170])\n",
      "epoch 7, loss [185.37656], train tau 0.22479969263076782, test_tau 0.3077999949455261,train_ndcg@1 tensor([0.8090]), test_ndcg@1 tensor([0.7045])\n",
      "epoch 8, loss [181.99469], train tau 0.21079960465431213, test_tau 0.303600013256073,train_ndcg@1 tensor([0.8250]), test_ndcg@1 tensor([0.7090])\n",
      "epoch 9, loss [179.05162], train tau 0.20559969544410706, test_tau 0.3059999942779541,train_ndcg@1 tensor([0.8260]), test_ndcg@1 tensor([0.6960])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-250/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([500, 5, 16]) Y_train.shape torch.Size([500, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(500, 5, 16) (500, 5) (500,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-500', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-500', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [439.07083], train tau 0.27480024099349976, test_tau 0.3027999997138977,train_ndcg@1 tensor([0.7715]), test_ndcg@1 tensor([0.7310])\n",
      "epoch 1, loss [404.63934], train tau 0.26160019636154175, test_tau 0.292199969291687,train_ndcg@1 tensor([0.7835]), test_ndcg@1 tensor([0.7375])\n",
      "epoch 2, loss [394.5865], train tau 0.25800037384033203, test_tau 0.29079991579055786,train_ndcg@1 tensor([0.7965]), test_ndcg@1 tensor([0.7290])\n",
      "epoch 3, loss [388.3881], train tau 0.24680030345916748, test_tau 0.28679993748664856,train_ndcg@1 tensor([0.8045]), test_ndcg@1 tensor([0.7460])\n",
      "epoch 4, loss [382.4903], train tau 0.24260008335113525, test_tau 0.2877999246120453,train_ndcg@1 tensor([0.8075]), test_ndcg@1 tensor([0.7490])\n",
      "epoch 5, loss [377.9009], train tau 0.23580020666122437, test_tau 0.2871999144554138,train_ndcg@1 tensor([0.8185]), test_ndcg@1 tensor([0.7460])\n",
      "epoch 6, loss [373.90753], train tau 0.22960031032562256, test_tau 0.28199997544288635,train_ndcg@1 tensor([0.8225]), test_ndcg@1 tensor([0.7510])\n",
      "epoch 7, loss [370.64905], train tau 0.2228003740310669, test_tau 0.2833999693393707,train_ndcg@1 tensor([0.8250]), test_ndcg@1 tensor([0.7515])\n",
      "epoch 8, loss [367.10226], train tau 0.2214004099369049, test_tau 0.2869999408721924,train_ndcg@1 tensor([0.8315]), test_ndcg@1 tensor([0.7445])\n",
      "epoch 9, loss [363.68634], train tau 0.2174004316329956, test_tau 0.2903999984264374,train_ndcg@1 tensor([0.8330]), test_ndcg@1 tensor([0.7395])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-500/result_summary.pkl\n",
      "Generate samples...\n",
      "use_weak_labels:False, we will use true labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-1000', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/n_train-1000', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [860.6048], train tau 0.27890074253082275, test_tau 0.2914000153541565,train_ndcg@1 tensor([0.7517]), test_ndcg@1 tensor([0.7330])\n",
      "epoch 1, loss [813.62115], train tau 0.26510095596313477, test_tau 0.2896000146865845,train_ndcg@1 tensor([0.7680]), test_ndcg@1 tensor([0.7380])\n",
      "epoch 2, loss [794.7994], train tau 0.2569010257720947, test_tau 0.2871999740600586,train_ndcg@1 tensor([0.7850]), test_ndcg@1 tensor([0.7470])\n",
      "epoch 3, loss [783.79266], train tau 0.2508009076118469, test_tau 0.2864000201225281,train_ndcg@1 tensor([0.7862]), test_ndcg@1 tensor([0.7450])\n",
      "epoch 4, loss [773.9639], train tau 0.24540114402770996, test_tau 0.2895999550819397,train_ndcg@1 tensor([0.7965]), test_ndcg@1 tensor([0.7475])\n",
      "epoch 5, loss [766.2816], train tau 0.2431010901927948, test_tau 0.2864000201225281,train_ndcg@1 tensor([0.8033]), test_ndcg@1 tensor([0.7510])\n",
      "epoch 6, loss [759.81525], train tau 0.23710080981254578, test_tau 0.2879999876022339,train_ndcg@1 tensor([0.8045]), test_ndcg@1 tensor([0.7510])\n",
      "epoch 7, loss [753.5138], train tau 0.23380091786384583, test_tau 0.2911999821662903,train_ndcg@1 tensor([0.8050]), test_ndcg@1 tensor([0.7380])\n",
      "epoch 8, loss [748.2453], train tau 0.23290088772773743, test_tau 0.296999990940094,train_ndcg@1 tensor([0.8043]), test_ndcg@1 tensor([0.7255])\n",
      "epoch 9, loss [741.2296], train tau 0.2303008735179901, test_tau 0.29819998145103455,train_ndcg@1 tensor([0.8090]), test_ndcg@1 tensor([0.7260])\n",
      "The experiment result is saved in ../../../tmp/results/default/n_train-1000/result_summary.pkl\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    for n_train in [50, 100, 250, 500, 1000]:\n",
    "        config_file_path = '{}/configs/extended-imdb-tmdb_ranking_experiment.yaml'.format(root_path)\n",
    "    \n",
    "        with open(config_file_path,'r') as conf_file:\n",
    "            conf = yaml.full_load(conf_file)\n",
    "            conf['project_root'] = root_path \n",
    "    \n",
    "        exp_name = f'n_train-{n_train}'\n",
    "        # fix configuration\n",
    "        conf['data_conf']['n_train'] = n_train\n",
    "        conf['data_conf']['n_test'] = 500\n",
    "        conf['data_conf']['processed_data_path'] = os.path.join(conf['data_conf']['processed_data_path'],\n",
    "                                                                exp_name)\n",
    "        \n",
    "        # l2r_training_conf\n",
    "        conf['l2r_training_conf']['use_weak_labels'] = False\n",
    "        conf['l2r_training_conf']['model_checkpoint'] = os.path.join(conf['l2r_training_conf']['model_checkpoint'],\n",
    "                                                                exp_name)\n",
    "        \n",
    "        # weak_sup_conf\n",
    "        conf['weak_sup_conf']['checkpoint_path'] = os.path.join(conf['weak_sup_conf']['checkpoint_path'],\n",
    "                                                                exp_name)\n",
    "        # result_path\n",
    "        conf['results_path'] = os.path.join(conf['results_path'], exp_name)\n",
    "        \n",
    "        \n",
    "        data_conf = conf['data_conf']\n",
    "        weak_sup_conf = conf['weak_sup_conf'] # For partial ranking experiments, we should give\n",
    "        l2r_training_conf = conf['l2r_training_conf']\n",
    "        data_conf['project_root'] = root_path\n",
    "        \n",
    "        dataset= datasets_factory.create_dataset(data_conf)\n",
    "        dataset.create_samples()\n",
    "        \n",
    "        if l2r_training_conf['use_weak_labels']:\n",
    "            Y_tilde, thetas = get_weak_labels(dataset, weak_sup_conf, root_path=root_path)\n",
    "            r_utils = RankingUtils(data_conf['dimension'])\n",
    "            kt = r_utils.mean_kt_distance(Y_tilde,dataset.Y)\n",
    "            print('kt distance: ', kt)\n",
    "            dataset.set_Y_tilde(Y_tilde)\n",
    "        else:\n",
    "            kt = None\n",
    "            \n",
    "        ptwrapper = PtrankingWrapper(data_conf=data_conf, weak_sup_conf=weak_sup_conf,\n",
    "                             l2r_training_conf=l2r_training_conf, result_path=conf['results_path'],\n",
    "                             wl_kt_distance = kt)\n",
    "        X_train, X_test, Y_train, Y_test = dataset.get_train_test_torch(use_weak_labels=l2r_training_conf['use_weak_labels'])\n",
    "        ptwrapper.set_data(X_train=X_train, X_test=X_test,\n",
    "                          Y_train=Y_train, Y_test=Y_test)\n",
    "        model = ptwrapper.get_model()\n",
    "        result = ptwrapper.train_model(model, verbose=1)\n",
    "        \n",
    "#         result_path = os.path.join('results', f\"smalltrue_{int(n_train)}_{seed}.pickle\")\n",
    "#         with open(result_path, 'wb') as f:\n",
    "#             pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-functionality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-cardinality",
   "language": "python",
   "name": "ws-cardinality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
