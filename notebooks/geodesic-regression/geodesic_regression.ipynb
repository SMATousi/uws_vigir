{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import geoopt\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from geoopt.manifolds.lorentz import Lorentz\n",
    "from geoopt.manifolds.lorentz import math\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_params(dim, zero=False):\n",
    "    # Generate model parameters\n",
    "    p = torch.randn((1, dim)) * 2.0\n",
    "    if zero:\n",
    "        p /= 1000\n",
    "    p = man.projx(p) # project onto the hyperboloid \n",
    "    man._check_point_on_manifold(p[0])\n",
    "    orig = man.origin((1, dim))\n",
    "    beta = torch.randn((1, dim)) * 2.0\n",
    "    if zero:\n",
    "        beta /= 1000\n",
    "    beta /= beta.norm(dim=-1)\n",
    "    beta = man.proju(orig, beta)\n",
    "    beta = man.transp(orig, p, beta)\n",
    "    return p, beta\n",
    "\n",
    "def plot_params(p, beta):\n",
    "    p_np = p.numpy()\n",
    "    beta_np = beta.numpy()\n",
    "    lins_x = np.linspace(-3, 3, 100)\n",
    "    linx_y = np.sqrt(k + lins_x**2)\n",
    "    plt.plot(lins_x, linx_y, color=\"C1\", label=\"Hyperboloid\")\n",
    "    plt.scatter(p_np[:, 1], p_np[:, 0], label=\"intercept p\")\n",
    "    plt.arrow(p_np[0, 1], p_np[0, 0], beta_np[0, 1], beta_np[0, 0], \n",
    "        width=0.01, color=\"C2\", label=\"weight beta\")\n",
    "    plt.legend()\n",
    "\n",
    "def add_noise(y, dim, sig=1.0):\n",
    "    n = y.shape[0]\n",
    "    orig = man.origin((1, dim))\n",
    "    eps = torch.randn(n, dim) * sig\n",
    "    eps = man.proju(orig, eps)\n",
    "    eps = man.transp(orig, y, eps)\n",
    "    ynoise = man.expmap(y, eps)\n",
    "    return ynoise\n",
    "\n",
    "def generate_data(n, p, beta, dim):\n",
    "    x = (torch.rand(n, 1) * 10.0) - 5.0\n",
    "    y = man.expmap(p, x * beta)\n",
    "    ynoise = add_noise(y, dim, sig=1.0)\n",
    "    return x, ynoise, y\n",
    "\n",
    "k = 50 # choose curvature parameter\n",
    "d = 80\n",
    "dim = d + 1\n",
    "man = Lorentz(k=k)\n",
    "n_train = 100 # 1000\n",
    "n_test = 100 # 500\n",
    "n = n_train + n_test\n",
    "p, beta = sample_params(dim)\n",
    "x, y, y_true = generate_data(n, p, beta, dim)\n",
    "\n",
    "if d == 1:\n",
    "    plot_params(p, beta)\n",
    "    plt.scatter(y[:, 1], y[:, 0], alpha=0.1, color='C3', label='data')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeodesicRegressor():\n",
    "    def __init__(self, K, dim, man):\n",
    "        self.K = K\n",
    "        self.dim = dim\n",
    "        self.man = man\n",
    "        q, alpha = sample_params(dim, zero=True)\n",
    "        self.q = torch.autograd.Variable(q, requires_grad=True)\n",
    "        self.alpha = torch.autograd.Variable(alpha, requires_grad=True)\n",
    "\n",
    "    def fit(self, X, Y, epochs=300, lr=0.01, verbose=False):\n",
    "        n = X.shape[0]\n",
    "\n",
    "        dataset = TensorDataset(X, Y)\n",
    "        dataloader = DataLoader(dataset, batch_size=n)\n",
    "        for e in range(epochs):\n",
    "            for X, Y in dataloader:\n",
    "                \n",
    "                Y_hat = self.man.expmap(self.q, X * self.alpha)\n",
    "                loss = torch.mean(self.man.dist2(Y, Y_hat))\n",
    "                #print(f\"Loss: {loss.item()}\")\n",
    "                \n",
    "                # Optimization step\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Get riemannian gradient \n",
    "                    dq = self.man.egrad2rgrad(self.q, self.q.grad)\n",
    "                    qnew = self.man.expmap(self.q, -lr * dq)\n",
    "                    # Gradient of alpha should be in the tangent space of q\n",
    "                    dalpha = self.man.egrad2rgrad(self.q, self.alpha.grad)\n",
    "                    alphanew = self.alpha - (lr * dalpha)\n",
    "                    alphanew = self.man.transp(self.q, qnew, alphanew)\n",
    "\n",
    "                    self.alpha.zero_()\n",
    "                    self.alpha.add_(alphanew)\n",
    "                    self.q.zero_()\n",
    "                    self.q.add_(qnew)\n",
    "\n",
    "                self.alpha.grad.zero_()\n",
    "                self.q.grad.zero_()\n",
    "\n",
    "            dataset = TensorDataset(X, Y)\n",
    "            dataloader = DataLoader(dataset, batch_size=n)\n",
    "            losses = []\n",
    "            for X, Y in dataloader:\n",
    "                with torch.no_grad():\n",
    "                    Y_hat = self.man.expmap(self.q, X * self.alpha)\n",
    "                    loss = torch.mean(self.man.dist2(Y, Y_hat))\n",
    "            losses.append(loss.detach().numpy())\n",
    "\n",
    "        return losses[-1]\n",
    "\n",
    "    def predict(self, X):\n",
    "        Y_hat = man.expmap(self.q, X * self.alpha)\n",
    "        return Y_hat\n",
    "\n",
    "model = GeodesicRegressor(k, dim, man)\n",
    "train_loss =  model.fit(x[:n_train], y[:n_train])\n",
    "yhat = model.predict(x[n_train:])\n",
    "val_loss = torch.mean(man.dist2(y[n_train:], yhat))\n",
    "print(f'Training loss: {train_loss:.4}')\n",
    "print(f\"Validation loss:{val_loss:.4}\")\n",
    "\n",
    "if d == 1:\n",
    "    plot_params(\n",
    "        model.q.detach(), \n",
    "        model.alpha.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_sizes = range(10, n_train+1, 10)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "pq_norms = []\n",
    "ba_norms = []\n",
    "\n",
    "for n_subset in subset_sizes:\n",
    "    model = GeodesicRegressor(k, dim, man)\n",
    "    train_loss = model.fit(x[:n_subset], y[:n_subset])\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    pq_norms.append(\n",
    "        np.linalg.norm(p[0] - model.q.detach().numpy()[0], ord=2))\n",
    "    ba_norms.append(\n",
    "        np.linalg.norm(beta[0] - model.alpha.detach().numpy()[0], ord=2))\n",
    "\n",
    "    yhat = model.predict(x[n_train:])\n",
    "    val_loss = torch.mean(man.dist2(y[n_train:], yhat))\n",
    "    val_losses.append(val_loss.detach().numpy())\n",
    "    print(f\"{n_subset} examples... Validation loss:{val_loss}\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].plot(subset_sizes, train_losses, label='Train loss')\n",
    "ax[0].plot(subset_sizes, val_losses, label='Validation loss')\n",
    "ax[0].set_xlabel('Number of training examples')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(subset_sizes, pq_norms, color='C3', \n",
    "    label=r'$\\|p - \\hat{p}\\|_2$')\n",
    "ax[1].plot(subset_sizes, ba_norms, color='C4', \n",
    "    label=r'$\\|\\beta - \\hat{\\beta}\\|_2$')\n",
    "ax[1].set_xlabel('Number of training examples')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFGenerator():\n",
    "    def __init__(self, num_lfs, siglb=2, sigub=5, heterogeneous=True):\n",
    "        self.siglb = siglb\n",
    "        self.sigub = sigub\n",
    "        self.heterogeneous = heterogeneous\n",
    "        self.num_lfs = num_lfs\n",
    "        self.sigs = []\n",
    "\n",
    "    def fit(self, sep=15):\n",
    "        for i in range(self.num_lfs):\n",
    "            coin = np.random.randint(0, 2) * self.heterogeneous\n",
    "            sig = np.random.uniform(\n",
    "                self.siglb + (coin*sep), self.sigub + (coin*sep))\n",
    "            self.sigs.append(sig)\n",
    "    \n",
    "    def predict(self, Y_true):\n",
    "        lambdas = []\n",
    "        for i in range(self.num_lfs):\n",
    "            lambda_i_preds = add_noise(Y_true, dim, sig=self.sigs[i])\n",
    "            lambdas.append(lambda_i_preds)\n",
    "        return lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class GeodesicLabelModel():\n",
    "    def __init__(self, num_triplets=1):\n",
    "        self.num_triplets = num_triplets\n",
    "        self.num_lfs = self.num_triplets * 3\n",
    "        self.thetas = []\n",
    "\n",
    "    def fit(self, lambdas):\n",
    "        dim = lambdas[0].shape[1]\n",
    "        for i in range(self.num_triplets):\n",
    "            Eab = torch.mean(\n",
    "                man.dist(lambdas[(i*3)+0], lambdas[(i*3)+1])) / dim\n",
    "            Ebc = torch.mean(\n",
    "                man.dist(lambdas[(i*3)+1], lambdas[(i*3)+2])) / dim\n",
    "            Eac = torch.mean(\n",
    "                man.dist(lambdas[(i*3)+0], lambdas[(i*3)+2])) / dim\n",
    "            E = np.array([Eab, Ebc, Eac])\n",
    "            coef = np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]])\n",
    "            thetas_abc = np.reciprocal(np.linalg.solve(coef, E))\n",
    "            self.thetas += list(thetas_abc)\n",
    "\n",
    "    def predict(self, lambdas, max_steps=200, lr=0.001, \n",
    "            verbose=False, vote=False):\n",
    "        n = lambdas[0].shape[0]\n",
    "        ys = torch.zeros_like(lambdas[0])\n",
    "        for i in tqdm(range(n)):\n",
    "            if vote:\n",
    "                thetas = np.ones_like(self.thetas)\n",
    "            else:\n",
    "                thetas = self.thetas\n",
    "                \n",
    "            # Initialize y to be on the manifold\n",
    "            y = torch.randn_like(lambdas[0][0])\n",
    "            y = torch.autograd.Variable(man.projx(y), requires_grad=True) \n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                objective = torch.zeros(1)\n",
    "                for j in range(self.num_lfs):\n",
    "                    objective += (\n",
    "                        thetas[j] * man.dist2(lambdas[j][i], y))\n",
    "                objective.backward()\n",
    "\n",
    "                # Riemannian descent step\n",
    "                with torch.no_grad():\n",
    "                    dy = man.egrad2rgrad(y, y.grad)\n",
    "                    ynew = man.expmap(y, -lr * dy)\n",
    "                    y.zero_()\n",
    "                    y.add_(ynew)\n",
    "                y.grad.zero_()\n",
    "            \n",
    "                if verbose:\n",
    "                    print(i, step, objective.item())\n",
    "            \n",
    "            ys[i] += y\n",
    "        return ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lambdas = 3\n",
    "lfGenerator = LFGenerator(num_lambdas, siglb=1.5, sigub=2.0) # TODO\n",
    "lfGenerator.fit()\n",
    "lambdas = lfGenerator.predict(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelModel = GeodesicLabelModel(num_triplets=num_lambdas//3)\n",
    "lambdas_train = [lambdas[i][:n_train, :] for i in range(len(lambdas))]\n",
    "labelModel.fit(lambdas_train) # TODO only fit label model with train examples? \n",
    "weak_labels = labelModel.predict(lambdas, max_steps=50, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_loss(n_clean):\n",
    "    #n_clean = n_train // 4\n",
    "    model = GeodesicRegressor(k, dim, man)\n",
    "    train_loss = model.fit(x[:n_clean], y[:n_clean])\n",
    "    yhat = model.predict(x[n_train:])\n",
    "    val_loss = torch.mean(man.dist2(y[n_train:], yhat))\n",
    "    val_loss = val_loss.detach().numpy()\n",
    "    print(f\"Validation loss clean: {val_loss}\")\n",
    "    return val_loss\n",
    "\n",
    "def get_ws_loss(n_clean, weak_labels):\n",
    "    weak_labels = torch.Tensor(weak_labels.detach().numpy())\n",
    "    model = GeodesicRegressor(k, dim, man)\n",
    "    extended_labels = torch.vstack(\n",
    "        [y[:n_clean], weak_labels[n_clean:n_train]])\n",
    "    train_loss = model.fit(x[:n_train], extended_labels)\n",
    "    yhat = model.predict(x[n_train:])\n",
    "    val_loss = torch.mean(man.dist2(y[n_train:], yhat))\n",
    "    val_loss = val_loss.detach().numpy()\n",
    "    print(f\"Validation loss weakly supervised: {val_loss}\")\n",
    "    return val_loss\n",
    "\n",
    "def get_ws_loss_lambdas(n_clean, num_lambdas, \n",
    "        siglb=1.5, sigub=2.0, vote=False, max_steps=100):\n",
    "    lfGenerator = LFGenerator(num_lambdas, siglb=siglb, sigub=sigub)\n",
    "    lfGenerator.fit()\n",
    "    lambdas = lfGenerator.predict(y_true)\n",
    "    lambdas_train = [lambdas[i][:n_train, :] for i in range(len(lambdas))]\n",
    "    \n",
    "    if vote:\n",
    "        labelModel = GeodesicLabelModel(num_triplets=num_lambdas//3)\n",
    "        labelModel.fit(lambdas_train) \n",
    "        \n",
    "        weak_labels = labelModel.predict(lambdas_train, max_steps=max_steps, verbose=False, vote=False)\n",
    "        ws_loss = get_ws_loss(n_clean, weak_labels)\n",
    "        weak_labels = labelModel.predict(lambdas_train, max_steps=max_steps, verbose=False, vote=True)\n",
    "        vote_loss = get_ws_loss(n_clean, weak_labels)\n",
    "\n",
    "        return ws_loss, labelModel, vote_loss\n",
    "    else:\n",
    "        labelModel = GeodesicLabelModel(num_triplets=num_lambdas//3)\n",
    "        labelModel.fit(lambdas_train) \n",
    "        weak_labels = labelModel.predict(lambdas_train, max_steps=max_steps, verbose=False, vote=False)\n",
    "        return get_ws_loss(n_clean, weak_labels), labelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clean_loss(n_train // 2)\n",
    "get_ws_loss(n_train // 2, weak_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fracs = [80, 90, n_train] \n",
    "\n",
    "clean_losses = []\n",
    "for i in range(len(fracs)):\n",
    "    loss = get_clean_loss(fracs[i])\n",
    "    clean_losses.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lfs = [3, 6, 9, 12, 15, 18, 21]\n",
    "ws_losses = []\n",
    "vote_losses = []\n",
    "label_models = []\n",
    "\n",
    "siglb = 1.5\n",
    "\n",
    "for num_lf in num_lfs:\n",
    "    ws_loss, label_model, vote_loss = get_ws_loss_lambdas(0, num_lf, \n",
    "        siglb=siglb, sigub=siglb+4.0, vote=True, max_steps=100)\n",
    "    ws_losses.append(ws_loss)\n",
    "    label_models.append(label_model)\n",
    "    vote_losses.append(vote_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = '25'\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3.5, 3.5))\n",
    "\n",
    "for i in range(len(fracs)):\n",
    "    ax.axhline(clean_losses[i], linestyle=':', color=f'C{i}', linewidth=4,  \n",
    "        label=f'Fully supervised ({int((fracs[i]/n_train)*100)}%)')\n",
    "\n",
    "ax.plot(num_lfs, ws_losses, '-v', label='WS (Ours)',\n",
    "    linewidth=4, markeredgewidth=4, color='C3')\n",
    "ax.plot(num_lfs, vote_losses, '-o', label='WS (Majority Vote)',\n",
    "    linewidth=4, markeredgewidth=4, color='C4')\n",
    "\n",
    "print(fracs)\n",
    "print(clean_losses)\n",
    "print(num_lfs)\n",
    "print(ws_losses)\n",
    "print(vote_losses)\n",
    "\n",
    "\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('Num LFs')\n",
    "#ax.yscale('log')\n",
    "ax.grid()\n",
    "ax.legend(bbox_to_anchor=(0.92, 0.92),\\\n",
    "    bbox_transform=plt.gcf().transFigure)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('geodesic_numLFs.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# average theta exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglbs = [1.5, 14.0] #3.0, 6.0, 12.0]\n",
    "num_lf = 9\n",
    "ws_losses = []\n",
    "label_models = []\n",
    "\n",
    "vote_losses = []\n",
    "for siglb in siglbs:\n",
    "    # Label model and vote\n",
    "    ws_loss, label_model, vote_loss = get_ws_loss_lambdas(0, num_lf, \n",
    "        siglb=siglb, sigub=siglb+4.0, vote=True, max_steps=100)\n",
    "    ws_losses.append(ws_loss)\n",
    "    label_models.append(label_model)\n",
    "    vote_losses.append(vote_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fracs)):\n",
    "    plt.axhline(clean_losses[i], linestyle=':', color=f'C{i}', \n",
    "        label=f'Fully supervised ({(fracs[i]/n_train)*100}%)')\n",
    "\n",
    "theta_averages = [np.mean(lm.thetas) for lm in label_models]\n",
    "plt.plot(theta_averages[:], ws_losses[:], '-v', label='WS (Our approach)', \n",
    "    color='C3')\n",
    "plt.plot(theta_averages[:], vote_losses[:], '-o', label='WS (Fréchet mean)', \n",
    "    color='C4')\n",
    "\n",
    "plt.ylabel('Geodesic loss')\n",
    "plt.xlabel('Theta average')\n",
    "plt.legend()\n",
    "plt.savefig('geodesic-theta.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75e1510848ff81b2a8a3022c3bfac472ed28a49a56e1422a056d525171f2408b"
  },
  "kernelspec": {
   "display_name": "universalizing-weak-supervision",
   "language": "python",
   "name": "universalizing-weak-supervision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
