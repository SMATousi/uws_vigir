{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complete-capital",
   "metadata": {},
   "source": [
    "# Path setup & import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "packed-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "root_path = '../../' # path to project root\n",
    "sys.path.append('{}/code'.format(root_path))\n",
    "sys.path.append('{}/code/core'.format(root_path))\n",
    "sys.path.append('{}/code/datasets/'.format(root_path))\n",
    "sys.path.insert(0,'{}/code/ptranking'.format(root_path))\n",
    "\n",
    "from core.ranking_utils import *\n",
    "from core.mallows import *\n",
    "from core.ws_ranking import *\n",
    "from core.ws_real_workflow import * \n",
    "from datasets.imdb_tmdb_dataset import * \n",
    "from datasets.basic_clmn_dataset import * \n",
    "from core.labelling.feature_lf import *\n",
    "from ptranking_wrapper import PtrankingWrapper\n",
    "import datasets_factory \n",
    "import numpy as np \n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-newcastle",
   "metadata": {},
   "source": [
    "# Read config & basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agreed-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '{}/configs/modified-mslr-web10k_ranking_experiment.yaml'.format(root_path)\n",
    "\n",
    "with open(config_file_path,'r') as conf_file:\n",
    "    conf = yaml.full_load(conf_file)\n",
    "    conf['project_root'] = root_path \n",
    "\n",
    "data_conf = conf['data_conf']\n",
    "weak_sup_conf = conf['weak_sup_conf'] # For partial ranking experiments, we should give\n",
    "l2r_training_conf = conf['l2r_training_conf']\n",
    "data_conf['project_root'] = root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-jersey",
   "metadata": {},
   "source": [
    "# Train and evaluation - mainly with PtrankingWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relative-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seed in range(5):\n",
    "#     kt = None\n",
    "#     save_path = os.path.join(root_path, 'data/MSLR-WEB10K/True_5')\n",
    "#     file_train = 'train.npz'\n",
    "#     file_test = 'test.npz'\n",
    "\n",
    "#     train = np.load(os.path.join(save_path, file_train))\n",
    "#     test = np.load(os.path.join(save_path, file_test))\n",
    "#     X_train, Y_train, qid_train = train['X'], train['Y'], train['qid']\n",
    "#     X_test, Y_test, qid_test = test['X'], test['Y'], test['qid']\n",
    "\n",
    "#     # Weak supervision\n",
    "#     d = X_train.shape[1]\n",
    "#     r_utils = RankingUtils(d)\n",
    "#     dummy_lf = FeatureRankingLF(rank_on_feature=0, d=d, highest_first=False)\n",
    "#     true_ranking = dummy_lf.apply_mat(np.concatenate((np.expand_dims(Y_train, axis=-1), np.expand_dims(Y_test, axis=-1))))\n",
    "#     lf = FeatureRankingLF(rank_on_feature=106, d=d, highest_first=True)\n",
    "#     wl = lf.apply_mat(np.concatenate((X_train, X_test)))\n",
    "#     kt = r_utils.mean_kt_distance(true_ranking, wl)\n",
    "#     wl_score = ranking_to_score(wl, d=d, highest_first=False)\n",
    "#     print('kt distance: ', kt)\n",
    "\n",
    "#     Y_train = wl_score[:len(X_train)]\n",
    "#     ptwrapper = PtrankingWrapper(data_conf=data_conf, weak_sup_conf=weak_sup_conf,\n",
    "#                                  l2r_training_conf=l2r_training_conf, result_path=conf['results_path'],\n",
    "#                                  wl_kt_distance = kt)\n",
    "#     ptwrapper.set_data(X_train=X_train, X_test=X_test,\n",
    "#                       Y_train=Y_train, Y_test=Y_test)\n",
    "#     model = ptwrapper.get_model()\n",
    "#     result = ptwrapper.train_model(model, IR=True, verbose=1)\n",
    "    \n",
    "#     result_path = f'results/BM25Only_{seed}.pickle'\n",
    "#     with open(result_path, 'wb') as f:\n",
    "#         pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sized-bones",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kt distance:  0.13176914778856524\n",
      "Training data shape, X_train.shape (1375, 5, 136) Y_train.shape torch.Size([1375, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1375, 5, 136) torch.Size([1375, 5]) (1375,)\n",
      "data_dict {'data_id': 'MODIFIED_MSLR_WEB10K', 'dir_data': 'data/MSLR-WEB10K/processed/default', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 136, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'MODIFIED_MSLR_WEB10K', 'dir_data': 'data/MSLR-WEB10K/processed/default', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 136, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(136, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=136, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [1079.8433], train tau 0.24829033017158508, test_tau 0.4108559489250183,train_ndcg@1 tensor([0.7718]), test_ndcg@1 tensor([0.6780])train_p@1 tensor([0.9724]), test_p@1, tensor([0.7996])\n",
      "epoch 1, loss [982.0532], train tau 0.2327996790409088, test_tau 0.39832985401153564,train_ndcg@1 tensor([0.7865]), test_ndcg@1 tensor([0.6962])train_p@1 tensor([0.9724]), test_p@1, tensor([0.7641])\n",
      "epoch 2, loss [947.64685], train tau 0.22421795129776, test_tau 0.38768270611763,train_ndcg@1 tensor([0.8065]), test_ndcg@1 tensor([0.7203])train_p@1 tensor([0.9709]), test_p@1, tensor([0.7683])\n",
      "epoch 3, loss [923.59186], train tau 0.21585413813591003, test_tau 0.39060544967651367,train_ndcg@1 tensor([0.8122]), test_ndcg@1 tensor([0.7119])train_p@1 tensor([0.9760]), test_p@1, tensor([0.7724])\n",
      "epoch 4, loss [909.10815], train tau 0.2089444100856781, test_tau 0.38538622856140137,train_ndcg@1 tensor([0.8160]), test_ndcg@1 tensor([0.7255])train_p@1 tensor([0.9767]), test_p@1, tensor([0.7516])\n",
      "epoch 5, loss [887.1292], train tau 0.20239880681037903, test_tau 0.3782881200313568,train_ndcg@1 tensor([0.8224]), test_ndcg@1 tensor([0.7082])train_p@1 tensor([0.9847]), test_p@1, tensor([0.7307])\n",
      "epoch 6, loss [872.54395], train tau 0.19694405794143677, test_tau 0.37974947690963745,train_ndcg@1 tensor([0.8289]), test_ndcg@1 tensor([0.7255])train_p@1 tensor([0.9862]), test_p@1, tensor([0.7432])\n",
      "epoch 7, loss [857.2131], train tau 0.18908929824829102, test_tau 0.38058456778526306,train_ndcg@1 tensor([0.8409]), test_ndcg@1 tensor([0.7354])train_p@1 tensor([0.9825]), test_p@1, tensor([0.7349])\n",
      "epoch 8, loss [843.3773], train tau 0.18203476071357727, test_tau 0.3774530291557312,train_ndcg@1 tensor([0.8444]), test_ndcg@1 tensor([0.7338])train_p@1 tensor([0.9840]), test_p@1, tensor([0.7620])\n",
      "epoch 9, loss [832.4802], train tau 0.18028903007507324, test_tau 0.3774530291557312,train_ndcg@1 tensor([0.8449]), test_ndcg@1 tensor([0.7286])train_p@1 tensor([0.9869]), test_p@1, tensor([0.7557])\n",
      "The experiment result is saved in ../../tmp/results/MSLR-WEB10K/default/result_summary.pkl\n",
      "kt distance:  0.13176914778856524\n",
      "Training data shape, X_train.shape (1375, 5, 136) Y_train.shape torch.Size([1375, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1375, 5, 136) torch.Size([1375, 5]) (1375,)\n",
      "data_dict {'data_id': 'MODIFIED_MSLR_WEB10K', 'dir_data': 'data/MSLR-WEB10K/processed/default', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 136, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'MODIFIED_MSLR_WEB10K', 'dir_data': 'data/MSLR-WEB10K/processed/default', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 136, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(136, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=136, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [1076.3715], train tau 0.24363592267036438, test_tau 0.39874738454818726,train_ndcg@1 tensor([0.7796]), test_ndcg@1 tensor([0.6832])train_p@1 tensor([0.9724]), test_p@1, tensor([0.7495])\n",
      "epoch 1, loss [984.7447], train tau 0.2255992293357849, test_tau 0.3903966546058655,train_ndcg@1 tensor([0.7933]), test_ndcg@1 tensor([0.7103])train_p@1 tensor([0.9702]), test_p@1, tensor([0.7516])\n",
      "epoch 2, loss [947.76733], train tau 0.21810832619667053, test_tau 0.38434237241744995,train_ndcg@1 tensor([0.8022]), test_ndcg@1 tensor([0.7077])train_p@1 tensor([0.9753]), test_p@1, tensor([0.7349])\n",
      "epoch 3, loss [925.265], train tau 0.2099629044532776, test_tau 0.3812108337879181,train_ndcg@1 tensor([0.8144]), test_ndcg@1 tensor([0.7234])train_p@1 tensor([0.9724]), test_p@1, tensor([0.7453])\n",
      "epoch 4, loss [909.13257], train tau 0.20385363698005676, test_tau 0.38371607661247253,train_ndcg@1 tensor([0.8171]), test_ndcg@1 tensor([0.7270])train_p@1 tensor([0.9767]), test_p@1, tensor([0.7495])\n",
      "epoch 5, loss [897.5977], train tau 0.1976715624332428, test_tau 0.38329851627349854,train_ndcg@1 tensor([0.8253]), test_ndcg@1 tensor([0.7166])train_p@1 tensor([0.9833]), test_p@1, tensor([0.7537])\n",
      "epoch 6, loss [883.55896], train tau 0.19207167625427246, test_tau 0.38413360714912415,train_ndcg@1 tensor([0.8347]), test_ndcg@1 tensor([0.7370])train_p@1 tensor([0.9796]), test_p@1, tensor([0.7662])\n",
      "epoch 7, loss [865.1102], train tau 0.1893807351589203, test_tau 0.38350728154182434,train_ndcg@1 tensor([0.8331]), test_ndcg@1 tensor([0.7265])train_p@1 tensor([0.9818]), test_p@1, tensor([0.7578])\n",
      "epoch 8, loss [856.6047], train tau 0.18778109550476074, test_tau 0.3780793249607086,train_ndcg@1 tensor([0.8340]), test_ndcg@1 tensor([0.7260])train_p@1 tensor([0.9855]), test_p@1, tensor([0.7453])\n",
      "epoch 9, loss [847.43286], train tau 0.181453138589859, test_tau 0.38517746329307556,train_ndcg@1 tensor([0.8433]), test_ndcg@1 tensor([0.7187])train_p@1 tensor([0.9847]), test_p@1, tensor([0.7745])\n",
      "The experiment result is saved in ../../tmp/results/MSLR-WEB10K/default/result_summary.pkl\n",
      "kt distance:  0.13176914778856524\n",
      "Training data shape, X_train.shape (1375, 5, 136) Y_train.shape torch.Size([1375, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1375, 5, 136) torch.Size([1375, 5]) (1375,)\n",
      "data_dict {'data_id': 'MODIFIED_MSLR_WEB10K', 'dir_data': 'data/MSLR-WEB10K/processed/default', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 136, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dict {'data_id': 'MODIFIED_MSLR_WEB10K', 'dir_data': 'data/MSLR-WEB10K/processed/default', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 136, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(136, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=136, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [1075.0133], train tau 0.2397812306880951, test_tau 0.3970772624015808,train_ndcg@1 tensor([0.7853]), test_ndcg@1 tensor([0.6978])train_p@1 tensor([0.9651]), test_p@1, tensor([0.7557])\n",
      "epoch 1, loss [974.5301], train tau 0.22749066352844238, test_tau 0.39874741435050964,train_ndcg@1 tensor([0.7973]), test_ndcg@1 tensor([0.6863])train_p@1 tensor([0.9753]), test_p@1, tensor([0.7516])\n",
      "epoch 2, loss [944.284], train tau 0.21854528784751892, test_tau 0.39436325430870056,train_ndcg@1 tensor([0.8018]), test_ndcg@1 tensor([0.6905])train_p@1 tensor([0.9724]), test_p@1, tensor([0.7537])\n",
      "epoch 3, loss [918.36206], train tau 0.20858073234558105, test_tau 0.39853864908218384,train_ndcg@1 tensor([0.8120]), test_ndcg@1 tensor([0.6900])train_p@1 tensor([0.9782]), test_p@1, tensor([0.7599])\n",
      "epoch 4, loss [900.5492], train tau 0.20494428277015686, test_tau 0.3924843370914459,train_ndcg@1 tensor([0.8215]), test_ndcg@1 tensor([0.7135])train_p@1 tensor([0.9775]), test_p@1, tensor([0.7620])\n",
      "epoch 5, loss [890.05005], train tau 0.1983262002468109, test_tau 0.3970772624015808,train_ndcg@1 tensor([0.8213]), test_ndcg@1 tensor([0.7035])train_p@1 tensor([0.9840]), test_p@1, tensor([0.7516])\n",
      "epoch 6, loss [875.5618], train tau 0.19468992948532104, test_tau 0.3960334360599518,train_ndcg@1 tensor([0.8285]), test_ndcg@1 tensor([0.6989])train_p@1 tensor([0.9862]), test_p@1, tensor([0.7557])\n",
      "epoch 7, loss [863.0313], train tau 0.19556251168251038, test_tau 0.4058455228805542,train_ndcg@1 tensor([0.8287]), test_ndcg@1 tensor([0.6874])train_p@1 tensor([0.9833]), test_p@1, tensor([0.7641])\n",
      "epoch 8, loss [848.0082], train tau 0.1851622760295868, test_tau 0.3968685269355774,train_ndcg@1 tensor([0.8411]), test_ndcg@1 tensor([0.6994])train_p@1 tensor([0.9869]), test_p@1, tensor([0.7557])\n",
      "epoch 9, loss [837.1075], train tau 0.18530809879302979, test_tau 0.4016701579093933,train_ndcg@1 tensor([0.8389]), test_ndcg@1 tensor([0.6989])train_p@1 tensor([0.9855]), test_p@1, tensor([0.7766])\n",
      "The experiment result is saved in ../../tmp/results/MSLR-WEB10K/default/result_summary.pkl\n",
      "kt distance:  0.13176914778856524\n",
      "Training data shape, X_train.shape (1375, 5, 136) Y_train.shape torch.Size([1375, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1375, 5, 136) torch.Size([1375, 5]) (1375,)\n",
      "data_dict {'data_id': 'MODIFIED_MSLR_WEB10K', 'dir_data': 'data/MSLR-WEB10K/processed/default', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 136, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'MODIFIED_MSLR_WEB10K', 'dir_data': 'data/MSLR-WEB10K/processed/default', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 136, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(136, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=136, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [1091.189], train tau 0.250326931476593, test_tau 0.3908141851425171,train_ndcg@1 tensor([0.7780]), test_ndcg@1 tensor([0.6952])train_p@1 tensor([0.9709]), test_p@1, tensor([0.7244])\n",
      "epoch 1, loss [986.422], train tau 0.2263268530368805, test_tau 0.37390393018722534,train_ndcg@1 tensor([0.7976]), test_ndcg@1 tensor([0.7135])train_p@1 tensor([0.9775]), test_p@1, tensor([0.7119])\n",
      "epoch 2, loss [944.9362], train tau 0.21578127145767212, test_tau 0.3778705596923828,train_ndcg@1 tensor([0.8056]), test_ndcg@1 tensor([0.7197])train_p@1 tensor([0.9804]), test_p@1, tensor([0.7286])\n",
      "epoch 3, loss [925.41034], train tau 0.21287214756011963, test_tau 0.3820459246635437,train_ndcg@1 tensor([0.8115]), test_ndcg@1 tensor([0.7234])train_p@1 tensor([0.9855]), test_p@1, tensor([0.7265])\n",
      "epoch 4, loss [898.0681], train tau 0.20341730117797852, test_tau 0.3862212896347046,train_ndcg@1 tensor([0.8231]), test_ndcg@1 tensor([0.7135])train_p@1 tensor([0.9825]), test_p@1, tensor([0.7411])\n",
      "epoch 5, loss [880.22797], train tau 0.19854441285133362, test_tau 0.38935282826423645,train_ndcg@1 tensor([0.8280]), test_ndcg@1 tensor([0.7119])train_p@1 tensor([0.9825]), test_p@1, tensor([0.7349])\n",
      "epoch 6, loss [862.19476], train tau 0.19563499093055725, test_tau 0.3872651159763336,train_ndcg@1 tensor([0.8276]), test_ndcg@1 tensor([0.7161])train_p@1 tensor([0.9840]), test_p@1, tensor([0.7328])\n",
      "epoch 7, loss [847.2742], train tau 0.1919989287853241, test_tau 0.38329851627349854,train_ndcg@1 tensor([0.8345]), test_ndcg@1 tensor([0.7197])train_p@1 tensor([0.9855]), test_p@1, tensor([0.7349])\n",
      "epoch 8, loss [849.8348], train tau 0.1970900297164917, test_tau 0.39331942796707153,train_ndcg@1 tensor([0.8338]), test_ndcg@1 tensor([0.7025])train_p@1 tensor([0.9840]), test_p@1, tensor([0.7349])\n",
      "epoch 9, loss [830.9771], train tau 0.18552595376968384, test_tau 0.38810020685195923,train_ndcg@1 tensor([0.8482]), test_ndcg@1 tensor([0.7114])train_p@1 tensor([0.9833]), test_p@1, tensor([0.7307])\n",
      "The experiment result is saved in ../../tmp/results/MSLR-WEB10K/default/result_summary.pkl\n",
      "kt distance:  0.13176914778856524\n",
      "Training data shape, X_train.shape (1375, 5, 136) Y_train.shape torch.Size([1375, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1375, 5, 136) torch.Size([1375, 5]) (1375,)\n",
      "data_dict {'data_id': 'MODIFIED_MSLR_WEB10K', 'dir_data': 'data/MSLR-WEB10K/processed/default', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 136, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'MODIFIED_MSLR_WEB10K', 'dir_data': 'data/MSLR-WEB10K/processed/default', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 136, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(136, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=136, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [1056.3091], train tau 0.24174556136131287, test_tau 0.39394575357437134,train_ndcg@1 tensor([0.7738]), test_ndcg@1 tensor([0.7046])train_p@1 tensor([0.9709]), test_p@1, tensor([0.7704])\n",
      "epoch 1, loss [972.7486], train tau 0.22538158297538757, test_tau 0.3837161064147949,train_ndcg@1 tensor([0.7976]), test_ndcg@1 tensor([0.7390])train_p@1 tensor([0.9775]), test_p@1, tensor([0.7537])\n",
      "epoch 2, loss [943.30255], train tau 0.21890825033187866, test_tau 0.38496869802474976,train_ndcg@1 tensor([0.8036]), test_ndcg@1 tensor([0.7208])train_p@1 tensor([0.9760]), test_p@1, tensor([0.7474])\n",
      "epoch 3, loss [918.7407], train tau 0.21127235889434814, test_tau 0.3828810155391693,train_ndcg@1 tensor([0.8136]), test_ndcg@1 tensor([0.7312])train_p@1 tensor([0.9745]), test_p@1, tensor([0.7349])\n",
      "epoch 4, loss [902.7591], train tau 0.20283561944961548, test_tau 0.37954071164131165,train_ndcg@1 tensor([0.8211]), test_ndcg@1 tensor([0.7307])train_p@1 tensor([0.9804]), test_p@1, tensor([0.7390])\n",
      "epoch 5, loss [883.8193], train tau 0.20065376162528992, test_tau 0.37432152032852173,train_ndcg@1 tensor([0.8276]), test_ndcg@1 tensor([0.7390])train_p@1 tensor([0.9796]), test_p@1, tensor([0.7620])\n",
      "epoch 6, loss [867.7183], train tau 0.19148984551429749, test_tau 0.37933194637298584,train_ndcg@1 tensor([0.8293]), test_ndcg@1 tensor([0.7370])train_p@1 tensor([0.9818]), test_p@1, tensor([0.7557])\n",
      "epoch 7, loss [852.77954], train tau 0.18603515625, test_tau 0.3822546899318695,train_ndcg@1 tensor([0.8353]), test_ndcg@1 tensor([0.7328])train_p@1 tensor([0.9840]), test_p@1, tensor([0.7641])\n",
      "epoch 8, loss [843.00244], train tau 0.18319866061210632, test_tau 0.37870562076568604,train_ndcg@1 tensor([0.8449]), test_ndcg@1 tensor([0.7312])train_p@1 tensor([0.9847]), test_p@1, tensor([0.7432])\n",
      "epoch 9, loss [827.3034], train tau 0.18341687321662903, test_tau 0.37974947690963745,train_ndcg@1 tensor([0.8449]), test_ndcg@1 tensor([0.7281])train_p@1 tensor([0.9884]), test_p@1, tensor([0.7537])\n",
      "The experiment result is saved in ../../tmp/results/MSLR-WEB10K/default/result_summary.pkl\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5, 10):\n",
    "    kt = None\n",
    "    save_path = os.path.join(root_path, 'data/MSLR-WEB10K/True_5')\n",
    "    file_train = 'train.npz'\n",
    "    file_test = 'test.npz'\n",
    "\n",
    "    train = np.load(os.path.join(save_path, file_train))\n",
    "    test = np.load(os.path.join(save_path, file_test))\n",
    "    X_train, Y_train, qid_train = train['X'], train['Y'], train['qid']\n",
    "    X_test, Y_test, qid_test = test['X'], test['Y'], test['qid']\n",
    "\n",
    "    # Weak supervision\n",
    "    d = X_train.shape[1]\n",
    "    r_utils = RankingUtils(d)\n",
    "    dummy_lf = FeatureRankingLF(rank_on_feature=0, d=d, highest_first=False)\n",
    "    true_ranking = dummy_lf.apply_mat(np.concatenate((np.expand_dims(Y_train, axis=-1), np.expand_dims(Y_test, axis=-1))))\n",
    "    lf = FeatureRankingLF(rank_on_feature=106, d=d, highest_first=True)\n",
    "    wl = lf.apply_mat(np.concatenate((X_train, X_test)))\n",
    "    kt = r_utils.mean_kt_distance(true_ranking, wl)\n",
    "    wl_score = ranking_to_score(wl, d=d, highest_first=False)\n",
    "    print('kt distance: ', kt)\n",
    "\n",
    "    Y_train = wl_score[:len(X_train)]\n",
    "    ptwrapper = PtrankingWrapper(data_conf=data_conf, weak_sup_conf=weak_sup_conf,\n",
    "                                 l2r_training_conf=l2r_training_conf, result_path=conf['results_path'],\n",
    "                                 wl_kt_distance = kt)\n",
    "    ptwrapper.set_data(X_train=X_train, X_test=X_test,\n",
    "                      Y_train=Y_train, Y_test=Y_test)\n",
    "    model = ptwrapper.get_model()\n",
    "    result = ptwrapper.train_model(model, IR=True, verbose=1)\n",
    "    \n",
    "    result_path = f'results/BM25Only_{seed}.pickle'\n",
    "    with open(result_path, 'wb') as f:\n",
    "        pickle.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-cardinality",
   "language": "python",
   "name": "ws-cardinality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
