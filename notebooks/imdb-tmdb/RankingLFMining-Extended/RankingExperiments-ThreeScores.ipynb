{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustained-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "root_path = '../../../' # path to project root\n",
    "sys.path.append('{}/code'.format(root_path))\n",
    "sys.path.append('{}/code/core'.format(root_path))\n",
    "sys.path.append('{}/code/datasets/'.format(root_path))\n",
    "sys.path.insert(0,'{}/code/ptranking'.format(root_path))\n",
    "\n",
    "from core.ranking_utils import *\n",
    "from core.mallows import *\n",
    "from core.ws_ranking import *\n",
    "from core.ws_real_workflow import * \n",
    "from datasets.imdb_tmdb_dataset import * \n",
    "from datasets.basic_clmn_dataset import * \n",
    "from core.labelling.feature_lf import *\n",
    "from ptranking_wrapper import PtrankingWrapper\n",
    "import datasets_factory \n",
    "import numpy as np \n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "southeast-bunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/extended-default/LFs/three/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.09386666666666667\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/three', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/three', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [864.21515], train tau 0.28750020265579224, test_tau 0.2993999719619751,train_ndcg@1 tensor([0.7437]), test_ndcg@1 tensor([0.7210])\n",
      "epoch 1, loss [817.76874], train tau 0.26350075006484985, test_tau 0.2919999957084656,train_ndcg@1 tensor([0.7632]), test_ndcg@1 tensor([0.7255])\n",
      "epoch 2, loss [798.57324], train tau 0.2548011243343353, test_tau 0.28439995646476746,train_ndcg@1 tensor([0.7835]), test_ndcg@1 tensor([0.7390])\n",
      "epoch 3, loss [785.59045], train tau 0.2491009533405304, test_tau 0.2874000072479248,train_ndcg@1 tensor([0.7883]), test_ndcg@1 tensor([0.7420])\n",
      "epoch 4, loss [775.7019], train tau 0.24250096082687378, test_tau 0.2856000065803528,train_ndcg@1 tensor([0.8018]), test_ndcg@1 tensor([0.7370])\n",
      "epoch 5, loss [769.4291], train tau 0.23940101265907288, test_tau 0.28299999237060547,train_ndcg@1 tensor([0.8060]), test_ndcg@1 tensor([0.7335])\n",
      "epoch 6, loss [762.52435], train tau 0.23430106043815613, test_tau 0.285800039768219,train_ndcg@1 tensor([0.8085]), test_ndcg@1 tensor([0.7405])\n",
      "epoch 7, loss [755.82166], train tau 0.23250052332878113, test_tau 0.28999999165534973,train_ndcg@1 tensor([0.8115]), test_ndcg@1 tensor([0.7450])\n",
      "epoch 8, loss [749.5225], train tau 0.2294008731842041, test_tau 0.2881999611854553,train_ndcg@1 tensor([0.8158]), test_ndcg@1 tensor([0.7395])\n",
      "epoch 9, loss [743.4494], train tau 0.2265009582042694, test_tau 0.2881999611854553,train_ndcg@1 tensor([0.8202]), test_ndcg@1 tensor([0.7345])\n",
      "The experiment result is saved in ../../../tmp/results/default/three/result_summary.pkl\n",
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/extended-default/LFs/three/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.09386666666666667\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/three', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/three', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [870.61914], train tau 0.2814004123210907, test_tau 0.29420003294944763,train_ndcg@1 tensor([0.7465]), test_ndcg@1 tensor([0.7445])\n",
      "epoch 1, loss [814.74713], train tau 0.2658005356788635, test_tau 0.28440001606941223,train_ndcg@1 tensor([0.7640]), test_ndcg@1 tensor([0.7450])\n",
      "epoch 2, loss [796.28186], train tau 0.2580007314682007, test_tau 0.27960002422332764,train_ndcg@1 tensor([0.7772]), test_ndcg@1 tensor([0.7425])\n",
      "epoch 3, loss [783.6692], train tau 0.24940085411071777, test_tau 0.2789999842643738,train_ndcg@1 tensor([0.7857]), test_ndcg@1 tensor([0.7470])\n",
      "epoch 4, loss [776.96765], train tau 0.24400073289871216, test_tau 0.27699998021125793,train_ndcg@1 tensor([0.7937]), test_ndcg@1 tensor([0.7590])\n",
      "epoch 5, loss [768.79395], train tau 0.240200936794281, test_tau 0.2752000093460083,train_ndcg@1 tensor([0.8005]), test_ndcg@1 tensor([0.7555])\n",
      "epoch 6, loss [760.54816], train tau 0.2356010377407074, test_tau 0.27820003032684326,train_ndcg@1 tensor([0.8055]), test_ndcg@1 tensor([0.7455])\n",
      "epoch 7, loss [754.65027], train tau 0.23330086469650269, test_tau 0.2825999855995178,train_ndcg@1 tensor([0.8085]), test_ndcg@1 tensor([0.7370])\n",
      "epoch 8, loss [751.0348], train tau 0.22710078954696655, test_tau 0.28360000252723694,train_ndcg@1 tensor([0.8095]), test_ndcg@1 tensor([0.7375])\n",
      "epoch 9, loss [742.0439], train tau 0.22370094060897827, test_tau 0.28440001606941223,train_ndcg@1 tensor([0.8145]), test_ndcg@1 tensor([0.7420])\n",
      "The experiment result is saved in ../../../tmp/results/default/three/result_summary.pkl\n",
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/extended-default/LFs/three/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.09386666666666667\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/three', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/three', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [859.53656], train tau 0.2750004529953003, test_tau 0.30080002546310425,train_ndcg@1 tensor([0.7585]), test_ndcg@1 tensor([0.7220])\n",
      "epoch 1, loss [802.9465], train tau 0.2633003294467926, test_tau 0.2946000099182129,train_ndcg@1 tensor([0.7720]), test_ndcg@1 tensor([0.7300])\n",
      "epoch 2, loss [786.4833], train tau 0.25520065426826477, test_tau 0.2889999747276306,train_ndcg@1 tensor([0.7780]), test_ndcg@1 tensor([0.7440])\n",
      "epoch 3, loss [775.85315], train tau 0.2497006058692932, test_tau 0.28939998149871826,train_ndcg@1 tensor([0.7857]), test_ndcg@1 tensor([0.7355])\n",
      "epoch 4, loss [768.53766], train tau 0.2457006573677063, test_tau 0.28859999775886536,train_ndcg@1 tensor([0.7895]), test_ndcg@1 tensor([0.7255])\n",
      "epoch 5, loss [762.0388], train tau 0.23930075764656067, test_tau 0.2897999882698059,train_ndcg@1 tensor([0.7962]), test_ndcg@1 tensor([0.7370])\n",
      "epoch 6, loss [755.9141], train tau 0.23670059442520142, test_tau 0.2889999747276306,train_ndcg@1 tensor([0.7933]), test_ndcg@1 tensor([0.7340])\n",
      "epoch 7, loss [750.75354], train tau 0.23390096426010132, test_tau 0.29020002484321594,train_ndcg@1 tensor([0.7980]), test_ndcg@1 tensor([0.7330])\n",
      "epoch 8, loss [745.5146], train tau 0.22900107502937317, test_tau 0.2955999970436096,train_ndcg@1 tensor([0.7990]), test_ndcg@1 tensor([0.7280])\n",
      "epoch 9, loss [741.5216], train tau 0.22880086302757263, test_tau 0.29600000381469727,train_ndcg@1 tensor([0.8067]), test_ndcg@1 tensor([0.7235])\n",
      "The experiment result is saved in ../../../tmp/results/default/three/result_summary.pkl\n",
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/extended-default/LFs/three/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.09386666666666667\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/three', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/three', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [852.11237], train tau 0.271600604057312, test_tau 0.2963999807834625,train_ndcg@1 tensor([0.7713]), test_ndcg@1 tensor([0.7305])\n",
      "epoch 1, loss [805.63763], train tau 0.26230067014694214, test_tau 0.29280000925064087,train_ndcg@1 tensor([0.7705]), test_ndcg@1 tensor([0.7340])\n",
      "epoch 2, loss [791.34064], train tau 0.257800817489624, test_tau 0.292199969291687,train_ndcg@1 tensor([0.7775]), test_ndcg@1 tensor([0.7340])\n",
      "epoch 3, loss [781.06165], train tau 0.25190067291259766, test_tau 0.2892000079154968,train_ndcg@1 tensor([0.7855]), test_ndcg@1 tensor([0.7285])\n",
      "epoch 4, loss [772.98035], train tau 0.24840089678764343, test_tau 0.2896000146865845,train_ndcg@1 tensor([0.7857]), test_ndcg@1 tensor([0.7375])\n",
      "epoch 5, loss [765.9262], train tau 0.2442011535167694, test_tau 0.28720003366470337,train_ndcg@1 tensor([0.7945]), test_ndcg@1 tensor([0.7375])\n",
      "epoch 6, loss [759.1871], train tau 0.2364012598991394, test_tau 0.28720003366470337,train_ndcg@1 tensor([0.8030]), test_ndcg@1 tensor([0.7335])\n",
      "epoch 7, loss [752.1492], train tau 0.23360103368759155, test_tau 0.28940004110336304,train_ndcg@1 tensor([0.8062]), test_ndcg@1 tensor([0.7380])\n",
      "epoch 8, loss [745.8291], train tau 0.22990083694458008, test_tau 0.287600040435791,train_ndcg@1 tensor([0.8058]), test_ndcg@1 tensor([0.7480])\n",
      "epoch 9, loss [741.59735], train tau 0.22850096225738525, test_tau 0.2850000262260437,train_ndcg@1 tensor([0.8148]), test_ndcg@1 tensor([0.7465])\n",
      "The experiment result is saved in ../../../tmp/results/default/three/result_summary.pkl\n",
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/extended-default/LFs/three/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.09386666666666667\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/three', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/three', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [877.2021], train tau 0.2891005575656891, test_tau 0.30100002884864807,train_ndcg@1 tensor([0.7487]), test_ndcg@1 tensor([0.7185])\n",
      "epoch 1, loss [819.79364], train tau 0.27300071716308594, test_tau 0.2903999984264374,train_ndcg@1 tensor([0.7542]), test_ndcg@1 tensor([0.7200])\n",
      "epoch 2, loss [803.10657], train tau 0.2623007595539093, test_tau 0.2802000641822815,train_ndcg@1 tensor([0.7688]), test_ndcg@1 tensor([0.7385])\n",
      "epoch 3, loss [791.66327], train tau 0.2562006711959839, test_tau 0.2800000309944153,train_ndcg@1 tensor([0.7788]), test_ndcg@1 tensor([0.7370])\n",
      "epoch 4, loss [782.6232], train tau 0.2522011399269104, test_tau 0.2833999991416931,train_ndcg@1 tensor([0.7768]), test_ndcg@1 tensor([0.7315])\n",
      "epoch 5, loss [776.165], train tau 0.24470114707946777, test_tau 0.27560001611709595,train_ndcg@1 tensor([0.7850]), test_ndcg@1 tensor([0.7460])\n",
      "epoch 6, loss [767.8508], train tau 0.2410012185573578, test_tau 0.2826000452041626,train_ndcg@1 tensor([0.7925]), test_ndcg@1 tensor([0.7485])\n",
      "epoch 7, loss [760.8277], train tau 0.23870140314102173, test_tau 0.2762000560760498,train_ndcg@1 tensor([0.7965]), test_ndcg@1 tensor([0.7600])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss [754.2547], train tau 0.2350013256072998, test_tau 0.27160000801086426,train_ndcg@1 tensor([0.7965]), test_ndcg@1 tensor([0.7545])\n",
      "epoch 9, loss [748.3059], train tau 0.23290103673934937, test_tau 0.2746000289916992,train_ndcg@1 tensor([0.8010]), test_ndcg@1 tensor([0.7665])\n",
      "The experiment result is saved in ../../../tmp/results/default/three/result_summary.pkl\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    config_file_path = '{}/configs/extended-imdb-tmdb_ranking_experiment_three.yaml'.format(root_path)\n",
    "    \n",
    "    with open(config_file_path,'r') as conf_file:\n",
    "        conf = yaml.full_load(conf_file)\n",
    "        conf['project_root'] = root_path \n",
    "        exp_name = f'three'\n",
    "        \n",
    "        # fix configuration\n",
    "        conf['data_conf']['n_train'] = 1000\n",
    "        conf['data_conf']['n_test'] = 500\n",
    "        conf['data_conf']['processed_data_path'] = os.path.join(conf['data_conf']['processed_data_path'],\n",
    "                                                                exp_name)\n",
    "        \n",
    "        # l2r_training_conf\n",
    "        conf['l2r_training_conf']['use_weak_labels'] = True\n",
    "        conf['l2r_training_conf']['model_checkpoint'] = os.path.join(conf['l2r_training_conf']['model_checkpoint'],\n",
    "                                                                exp_name)\n",
    "        \n",
    "        # weak_sup_conf\n",
    "        conf['weak_sup_conf']['checkpoint_path'] = os.path.join(conf['weak_sup_conf']['checkpoint_path'],\n",
    "                                                                exp_name)\n",
    "        # result_path\n",
    "        conf['results_path'] = os.path.join(conf['results_path'], exp_name)\n",
    "        \n",
    "        \n",
    "        data_conf = conf['data_conf']\n",
    "        weak_sup_conf = conf['weak_sup_conf'] # For partial ranking experiments, we should give\n",
    "        l2r_training_conf = conf['l2r_training_conf']\n",
    "        data_conf['project_root'] = root_path\n",
    "        \n",
    "        dataset= datasets_factory.create_dataset(data_conf)\n",
    "        dataset.create_samples()\n",
    "        \n",
    "        if l2r_training_conf['use_weak_labels']:\n",
    "            Y_tilde, thetas = get_weak_labels(dataset, weak_sup_conf, root_path=root_path)\n",
    "            r_utils = RankingUtils(data_conf['dimension'])\n",
    "            kt = r_utils.mean_kt_distance(Y_tilde,dataset.Y)\n",
    "            print('kt distance: ', kt)\n",
    "            dataset.set_Y_tilde(Y_tilde)\n",
    "        else:\n",
    "            kt = None\n",
    "            \n",
    "        ptwrapper = PtrankingWrapper(data_conf=data_conf, weak_sup_conf=weak_sup_conf,\n",
    "                             l2r_training_conf=l2r_training_conf, result_path=conf['results_path'],\n",
    "                             wl_kt_distance = kt)\n",
    "        X_train, X_test, Y_train, Y_test = dataset.get_train_test_torch(use_weak_labels=l2r_training_conf['use_weak_labels'])\n",
    "        ptwrapper.set_data(X_train=X_train, X_test=X_test,\n",
    "                          Y_train=Y_train, Y_test=Y_test)\n",
    "        model = ptwrapper.get_model()\n",
    "        result = ptwrapper.train_model(model, verbose=1)\n",
    "        \n",
    "#         result_path = os.path.join('results', f\"{exp_name}+_{seed}.pickle\")\n",
    "        result_path = os.path.join('results', f\"refactoring_test_{exp_name}+_{seed}.pickle\")\n",
    "        with open(result_path, 'wb') as f:\n",
    "            pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-pierre",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-cardinality",
   "language": "python",
   "name": "ws-cardinality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
