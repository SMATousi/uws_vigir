{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complete-capital",
   "metadata": {},
   "source": [
    "# Path setup & import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "packed-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "root_path = '../../../' # path to project root\n",
    "sys.path.append('{}/code'.format(root_path))\n",
    "sys.path.append('{}/code/core'.format(root_path))\n",
    "sys.path.append('{}/code/datasets/'.format(root_path))\n",
    "sys.path.insert(0,'{}/code/ptranking'.format(root_path))\n",
    "\n",
    "from core.ranking_utils import *\n",
    "from core.mallows import *\n",
    "from core.ws_ranking import *\n",
    "from core.ws_real_workflow import * \n",
    "from datasets.imdb_tmdb_dataset import * \n",
    "from datasets.basic_clmn_dataset import * \n",
    "from core.labelling.feature_lf import *\n",
    "from ptranking_wrapper import PtrankingWrapper\n",
    "import datasets_factory \n",
    "import numpy as np \n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-newcastle",
   "metadata": {},
   "source": [
    "# Read config & basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agreed-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '{}/configs/imdb-tmdb_ranking_experiment_play2.yaml'.format(root_path)\n",
    "\n",
    "with open(config_file_path,'r') as conf_file:\n",
    "    conf = yaml.full_load(conf_file)\n",
    "    conf['project_root'] = root_path \n",
    "\n",
    "data_conf = conf['data_conf']\n",
    "weak_sup_conf = conf['weak_sup_conf'] # For partial ranking experiments, we should give\n",
    "l2r_training_conf = conf['l2r_training_conf']\n",
    "data_conf['project_root'] = root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "attached-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_sup_conf['synthetic'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-jersey",
   "metadata": {},
   "source": [
    "# Train and evaluation - mainly with PtrankingWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "relative-press",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/LFs/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.34995000000000004\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([5000, 5, 255]) Y_train.shape torch.Size([5000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(5000, 5, 255) (5000, 5) (5000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(255, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=255, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [1184.0251], train tau 0.022727876901626587, test_tau 0.34759998321533203,train_ndcg@1 tensor([0.9754]), test_ndcg@1 tensor([0.6553])\n",
      "epoch 1, loss [581.49646], train tau 0.01356479525566101, test_tau 0.3492000102996826,train_ndcg@1 tensor([0.9827]), test_ndcg@1 tensor([0.6530])\n",
      "epoch 2, loss [452.09705], train tau 0.010783791542053223, test_tau 0.3492000102996826,train_ndcg@1 tensor([0.9863]), test_ndcg@1 tensor([0.6520])\n",
      "epoch 3, loss [379.22647], train tau 0.00880315899848938, test_tau 0.35040003061294556,train_ndcg@1 tensor([0.9884]), test_ndcg@1 tensor([0.6503])\n",
      "epoch 4, loss [327.78064], train tau 0.008343011140823364, test_tau 0.35019999742507935,train_ndcg@1 tensor([0.9891]), test_ndcg@1 tensor([0.6513])\n",
      "The experiment result is saved in ../../../tmp/results/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/result_summary.pkl\n",
      "0 [0.35040003]\n",
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/LFs/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.34995000000000004\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([5000, 5, 255]) Y_train.shape torch.Size([5000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(5000, 5, 255) (5000, 5) (5000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(255, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=255, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [1163.0955], train tau 0.022807776927947998, test_tau 0.34870001673698425,train_ndcg@1 tensor([0.9757]), test_ndcg@1 tensor([0.6525])\n",
      "epoch 1, loss [568.5618], train tau 0.013464689254760742, test_tau 0.3508000373840332,train_ndcg@1 tensor([0.9834]), test_ndcg@1 tensor([0.6520])\n",
      "epoch 2, loss [430.55673], train tau 0.01082378625869751, test_tau 0.3530000150203705,train_ndcg@1 tensor([0.9864]), test_ndcg@1 tensor([0.6470])\n",
      "epoch 3, loss [362.44415], train tau 0.008882999420166016, test_tau 0.352400004863739,train_ndcg@1 tensor([0.9886]), test_ndcg@1 tensor([0.6465])\n",
      "epoch 4, loss [317.52512], train tau 0.007902532815933228, test_tau 0.35270002484321594,train_ndcg@1 tensor([0.9895]), test_ndcg@1 tensor([0.6465])\n",
      "The experiment result is saved in ../../../tmp/results/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/result_summary.pkl\n",
      "1 [0.35300002]\n",
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/LFs/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.34995000000000004\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([5000, 5, 255]) Y_train.shape torch.Size([5000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(5000, 5, 255) (5000, 5) (5000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(255, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=255, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [1168.231], train tau 0.021727770566940308, test_tau 0.3475000262260437,train_ndcg@1 tensor([0.9762]), test_ndcg@1 tensor([0.6607])\n",
      "epoch 1, loss [561.65137], train tau 0.013404875993728638, test_tau 0.3499000072479248,train_ndcg@1 tensor([0.9837]), test_ndcg@1 tensor([0.6530])\n",
      "epoch 2, loss [433.47894], train tau 0.011124253273010254, test_tau 0.35110002756118774,train_ndcg@1 tensor([0.9859]), test_ndcg@1 tensor([0.6555])\n",
      "epoch 3, loss [367.28027], train tau 0.009203463792800903, test_tau 0.35100001096725464,train_ndcg@1 tensor([0.9876]), test_ndcg@1 tensor([0.6525])\n",
      "epoch 4, loss [323.3191], train tau 0.007823050022125244, test_tau 0.3516000211238861,train_ndcg@1 tensor([0.9897]), test_ndcg@1 tensor([0.6532])\n",
      "The experiment result is saved in ../../../tmp/results/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/result_summary.pkl\n",
      "2 [0.35160002]\n",
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/LFs/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.34995000000000004\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([5000, 5, 255]) Y_train.shape torch.Size([5000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(5000, 5, 255) (5000, 5) (5000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(255, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=255, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [1162.0007], train tau 0.022527724504470825, test_tau 0.3477000296115875,train_ndcg@1 tensor([0.9743]), test_ndcg@1 tensor([0.6535])\n",
      "epoch 1, loss [568.9075], train tau 0.013865232467651367, test_tau 0.350100040435791,train_ndcg@1 tensor([0.9832]), test_ndcg@1 tensor([0.6525])\n",
      "epoch 2, loss [438.07], train tau 0.011023640632629395, test_tau 0.3500000238418579,train_ndcg@1 tensor([0.9855]), test_ndcg@1 tensor([0.6538])\n",
      "epoch 3, loss [369.10434], train tau 0.008682847023010254, test_tau 0.3507000207901001,train_ndcg@1 tensor([0.9884]), test_ndcg@1 tensor([0.6538])\n",
      "epoch 4, loss [321.84082], train tau 0.0076826512813568115, test_tau 0.3507000207901001,train_ndcg@1 tensor([0.9896]), test_ndcg@1 tensor([0.6550])\n",
      "The experiment result is saved in ../../../tmp/results/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/result_summary.pkl\n",
      "3 [0.35070002]\n",
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/LFs/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.34995000000000004\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([5000, 5, 255]) Y_train.shape torch.Size([5000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(5000, 5, 255) (5000, 5) (5000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(255, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=255, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [1137.6051], train tau 0.023767918348312378, test_tau 0.34710001945495605,train_ndcg@1 tensor([0.9736]), test_ndcg@1 tensor([0.6575])\n",
      "epoch 1, loss [574.80725], train tau 0.014725059270858765, test_tau 0.35019999742507935,train_ndcg@1 tensor([0.9821]), test_ndcg@1 tensor([0.6492])\n",
      "epoch 2, loss [444.66245], train tau 0.011564105749130249, test_tau 0.3530000150203705,train_ndcg@1 tensor([0.9856]), test_ndcg@1 tensor([0.6430])\n",
      "epoch 3, loss [375.12585], train tau 0.00940316915512085, test_tau 0.3521000146865845,train_ndcg@1 tensor([0.9877]), test_ndcg@1 tensor([0.6438])\n",
      "epoch 4, loss [328.04932], train tau 0.00876307487487793, test_tau 0.35340002179145813,train_ndcg@1 tensor([0.9884]), test_ndcg@1 tensor([0.6395])\n",
      "The experiment result is saved in ../../../tmp/results/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/result_summary.pkl\n",
      "4 [0.35340002]\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    dataset= datasets_factory.create_dataset(data_conf)\n",
    "    dataset.create_samples()\n",
    "    \n",
    "    if l2r_training_conf['use_weak_labels']:\n",
    "        Y_tilde, thetas = get_weak_labels(dataset, weak_sup_conf, root_path=root_path)\n",
    "        r_utils = RankingUtils(data_conf['dimension'])\n",
    "        kt = r_utils.mean_kt_distance(Y_tilde,dataset.Y)\n",
    "        print('kt distance: ', kt)\n",
    "        dataset.set_Y_tilde(Y_tilde)\n",
    "    else:\n",
    "        kt = None\n",
    "    \n",
    "    ptwrapper = PtrankingWrapper(data_conf=data_conf, weak_sup_conf=weak_sup_conf,\n",
    "                                 l2r_training_conf=l2r_training_conf, result_path=conf['results_path'],\n",
    "                                 wl_kt_distance = kt)\n",
    "    X_train, X_test, Y_train, Y_test = dataset.get_train_test_torch(use_weak_labels=l2r_training_conf['use_weak_labels'])\n",
    "    ptwrapper.set_data(X_train=X_train, X_test=X_test,\n",
    "                      Y_train=Y_train, Y_test=Y_test)\n",
    "    model = ptwrapper.get_model()\n",
    "    result = ptwrapper.train_model(model, verbose=1)\n",
    "    \n",
    "    with open(os.path.join(f'results/with_imdbscore_seed_{seed}.pickle'), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "        \n",
    "    print(seed, max(result['test_tau']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-pressure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-latino",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-trustee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-drinking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-possession",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-abortion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-cardinality",
   "language": "python",
   "name": "ws-cardinality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
