{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecological-width",
   "metadata": {},
   "source": [
    "# Path setup & import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subtle-polymer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "root_path = '../../../' # path to project root\n",
    "sys.path.append('{}/code'.format(root_path))\n",
    "sys.path.append('{}/code/core'.format(root_path))\n",
    "sys.path.append('{}/code/datasets/'.format(root_path))\n",
    "sys.path.insert(0,'{}/code/ptranking'.format(root_path))\n",
    "\n",
    "from core.ranking_utils import *\n",
    "from core.mallows import *\n",
    "from core.ws_ranking import *\n",
    "from core.ws_real_workflow import * \n",
    "from core.ws_regression import *\n",
    "from datasets.imdb_tmdb_dataset import * \n",
    "from datasets.basic_clmn_dataset import * \n",
    "from core.labelling.feature_lf import *\n",
    "from ptranking_wrapper import PtrankingWrapper\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import datasets_factory \n",
    "import numpy as np \n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy\n",
    "import scipy.stats as ss\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "seed = 42\n",
    "feature_drop_cols = ['vote_average', 'imdbId', 'movieId','tmdbId']\n",
    "label_feature = 'vote_average'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-collection",
   "metadata": {},
   "source": [
    "# Fully supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "micro-seating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.37324481674300336\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv(os.path.join(root_path, 'data',\n",
    "                              'imdb-tmdb', 'merged_imdb_tmdb_with_additional_features.csv')).fillna(0)\n",
    "\n",
    "X, Y = df.drop(feature_drop_cols, axis=1), df[label_feature]\n",
    "\n",
    "# split data\n",
    "train_fraction = 0.75\n",
    "# # split data\n",
    "indices_train, indices_test = train_test_split(list(range(len(Y))), train_size=train_fraction, random_state=seed)\n",
    "X_train = X.iloc[indices_train]\n",
    "Y_train = Y[indices_train]\n",
    "X_test = X.iloc[indices_test]\n",
    "Y_test = Y[indices_test]\n",
    "\n",
    "# model declaration and fit\n",
    "model = Pipeline([\n",
    "    (\"std\", StandardScaler()),\n",
    "    (\"model\", GradientBoostingRegressor(n_estimators=250, random_state=seed))\n",
    "])\n",
    "model.fit(X_train, Y_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "true_mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "print('MSE', true_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-clear",
   "metadata": {},
   "source": [
    "# Weak Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "private-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_mean = Y.mean() # centering for LF generation\n",
    "# Y_var = Y.var()\n",
    "\n",
    "# param_err, mu_err, score, mse = {}, {}, {}, {}\n",
    "\n",
    "# for m in range(3, 20):\n",
    "#     n = len(Y)\n",
    "#     L, true_Sigma = generate_lfs(m, n, Y - Y_mean, Y_var)\n",
    "\n",
    "#     lm = LabelModel()\n",
    "#     lm.fit(L, Y_var)\n",
    "#     lm.inference(L)\n",
    "#     Y_hat = lm.Y_hat + Y_mean\n",
    "\n",
    "    \n",
    "#     Sigma_hat = lm.Sigma_hat\n",
    "#     param_err[m] = np.linalg.norm(Sigma_hat - true_Sigma) / (m+1)**2\n",
    "#     mu_err[m]    = np.linalg.norm(Sigma_hat[:m,m] - true_Sigma[:m,m]) / m\n",
    "#     score[m] = lm.score(Y - Y_mean)\n",
    "    \n",
    "    \n",
    "#     # # split data\n",
    "#     indices_train, indices_test = train_test_split(list(range(len(Y))), train_size=train_fraction, random_state=seed)\n",
    "#     X_train = X.iloc[indices_train]\n",
    "#     Y_train = Y_hat[indices_train]\n",
    "#     X_test = X.iloc[indices_test]\n",
    "#     Y_test = Y[indices_test]\n",
    "\n",
    "#     # model declaration and fit\n",
    "#     model = Pipeline([\n",
    "#         (\"std\", StandardScaler()),\n",
    "#         (\"model\", GradientBoostingRegressor(n_estimators=250, random_state=seed))\n",
    "#     ])\n",
    "#     model.fit(X_train, Y_train)\n",
    "#     Y_test_pred = model.predict(X_test)\n",
    "#     mse[m] = mean_squared_error(Y_test, Y_test_pred)\n",
    "#     print('numLFs', m, 'MSE', mse[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-register",
   "metadata": {},
   "source": [
    "# Fully supervision with small true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "operational-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame()\n",
    "\n",
    "for seed in range(10):\n",
    "    for sample_percent in [0.1, 1, 5, 10, 25, 50]:\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=train_fraction, random_state=seed)\n",
    "        sample_size = int(len(X_train) * sample_percent / 100)\n",
    "        np.random.seed(seed=seed)\n",
    "        sample_indices = np.random.choice(list(range(len(X_train))), size=sample_size, replace=False)\n",
    "        X_train = X_train.iloc[sample_indices]\n",
    "        Y_train = Y_train.iloc[sample_indices]\n",
    "\n",
    "        # model declaration and fit\n",
    "        model = Pipeline([\n",
    "            (\"std\", StandardScaler()),\n",
    "            (\"model\", GradientBoostingRegressor(n_estimators=250, random_state=seed))\n",
    "        ])\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        Y_test_pred = model.predict(X_test)\n",
    "        df_result = df_result.append({\n",
    "            'seed': seed,\n",
    "            'sample_percent': sample_percent,\n",
    "            'n_train': sample_size,\n",
    "            'mse': mean_squared_error(Y_test, Y_test_pred)\n",
    "        }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "indonesian-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.groupby(['sample_percent', 'n_train']).agg(['mean', 'std'])['mse'].to_csv('smalltrue_baselines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-chick",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-cardinality",
   "language": "python",
   "name": "ws-cardinality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
