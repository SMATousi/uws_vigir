{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complete-capital",
   "metadata": {},
   "source": [
    "# Path setup & import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "packed-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "root_path = '../../../' # path to project root\n",
    "sys.path.append('{}/code'.format(root_path))\n",
    "sys.path.append('{}/code/core'.format(root_path))\n",
    "sys.path.append('{}/code/datasets/'.format(root_path))\n",
    "sys.path.insert(0,'{}/code/ptranking'.format(root_path))\n",
    "\n",
    "from core.ranking_utils import *\n",
    "from core.mallows import *\n",
    "from core.ws_ranking import *\n",
    "from core.ws_real_workflow import * \n",
    "from datasets.imdb_tmdb_dataset import * \n",
    "from datasets.basic_clmn_dataset import * \n",
    "from core.labelling.feature_lf import *\n",
    "from ptranking_wrapper import PtrankingWrapper\n",
    "import datasets_factory \n",
    "import numpy as np \n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-newcastle",
   "metadata": {},
   "source": [
    "# Read config & basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agreed-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '{}/configs/imdb-tmdb_ranking_experiment_play.yaml'.format(root_path)\n",
    "\n",
    "with open(config_file_path,'r') as conf_file:\n",
    "    conf = yaml.full_load(conf_file)\n",
    "    conf['project_root'] = root_path \n",
    "\n",
    "data_conf = conf['data_conf']\n",
    "weak_sup_conf = conf['weak_sup_conf'] # For partial ranking experiments, we should give\n",
    "l2r_training_conf = conf['l2r_training_conf']\n",
    "data_conf['project_root'] = root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "attached-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_sup_conf['synthetic'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-jersey",
   "metadata": {},
   "source": [
    "# Train and evaluation - mainly with PtrankingWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "relative-press",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/LFs/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.3519333333333333\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([5000, 5, 255]) Y_train.shape torch.Size([5000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(5000, 5, 255) (5000, 5) (5000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(255, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=255, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [1576.3556], train tau 0.05007246136665344, test_tau 0.3546000123023987,train_ndcg@1 tensor([0.9631]), test_ndcg@1 tensor([0.6425])\n",
      "epoch 1, loss [1218.2473], train tau 0.045112550258636475, test_tau 0.3522999882698059,train_ndcg@1 tensor([0.9664]), test_ndcg@1 tensor([0.6465])\n",
      "epoch 2, loss [1150.098], train tau 0.04265296459197998, test_tau 0.3517000079154968,train_ndcg@1 tensor([0.9687]), test_ndcg@1 tensor([0.6480])\n",
      "epoch 3, loss [1107.8177], train tau 0.04067260026931763, test_tau 0.3517000079154968,train_ndcg@1 tensor([0.9701]), test_ndcg@1 tensor([0.6488])\n",
      "epoch 4, loss [1072.2288], train tau 0.039032042026519775, test_tau 0.35110002756118774,train_ndcg@1 tensor([0.9717]), test_ndcg@1 tensor([0.6515])\n",
      "The experiment result is saved in ../../../tmp/results/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/result_summary.pkl\n",
      "0 [0.3546]\n",
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/LFs/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.3519333333333333\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([5000, 5, 255]) Y_train.shape torch.Size([5000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(5000, 5, 255) (5000, 5) (5000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(255, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=255, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [1572.4852], train tau 0.049492835998535156, test_tau 0.35290002822875977,train_ndcg@1 tensor([0.9629]), test_ndcg@1 tensor([0.6500])\n",
      "epoch 1, loss [1235.2373], train tau 0.04459276795387268, test_tau 0.3517000079154968,train_ndcg@1 tensor([0.9676]), test_ndcg@1 tensor([0.6507])\n",
      "epoch 2, loss [1165.4662], train tau 0.04259222745895386, test_tau 0.35100001096725464,train_ndcg@1 tensor([0.9695]), test_ndcg@1 tensor([0.6482])\n",
      "epoch 3, loss [1119.537], train tau 0.040651947259902954, test_tau 0.35180002450942993,train_ndcg@1 tensor([0.9705]), test_ndcg@1 tensor([0.6485])\n",
      "epoch 4, loss [1079.4371], train tau 0.03845122456550598, test_tau 0.35040003061294556,train_ndcg@1 tensor([0.9721]), test_ndcg@1 tensor([0.6497])\n",
      "The experiment result is saved in ../../../tmp/results/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/result_summary.pkl\n",
      "1 [0.35290003]\n",
      "Generate samples...\n",
      "Weak labels generated and saved in ../../../data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False/LFs/weak_labels.pkl\n",
      "Use our weak supervision...train_method: triplet_opt,inference_rule: weighted_kemeny\n",
      "kt distance:  0.3519333333333333\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([5000, 5, 255]) Y_train.shape torch.Size([5000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(5000, 5, 255) (5000, 5) (5000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/210513_dim-10_ntrain-500_ntest-1000_model-ListMLE_weaklabel-False', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 255, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(255, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=255, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [1585.7478], train tau 0.050112903118133545, test_tau 0.3540000021457672,train_ndcg@1 tensor([0.9636]), test_ndcg@1 tensor([0.6457])\n",
      "epoch 1, loss [1229.1354], train tau 0.045732468366622925, test_tau 0.3531000018119812,train_ndcg@1 tensor([0.9664]), test_ndcg@1 tensor([0.6430])\n",
      "epoch 2, loss [1150.5269], train tau 0.04193222522735596, test_tau 0.3525000214576721,train_ndcg@1 tensor([0.9704]), test_ndcg@1 tensor([0.6435])\n",
      "epoch 3, loss [1097.8229], train tau 0.039971619844436646, test_tau 0.3529999852180481,train_ndcg@1 tensor([0.9715]), test_ndcg@1 tensor([0.6472])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5c9c5b519c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                       Y_train=Y_train, Y_test=Y_test)\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'results/seed_{seed}.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/universalizing-weak-supervision/code/core/ptranking_wrapper.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model, IR, verbose, model_save)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             ranker, result_summary = self.ltr_evaluator.custom_train(ranker=model, eval_dict=self.eval_dict,\n\u001b[0;32m--> 105\u001b[0;31m         train_data=self.train_data, test_data=self.test_data, verbose=verbose)\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mmodel_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'project_root'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2r_training_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_checkpoint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/universalizing-weak-supervision/code/ptranking/ptranking/ltr_adhoc/eval/ltr.py\u001b[0m in \u001b[0;36mcustom_train\u001b[0;34m(self, ranker, eval_dict, verbose, train_data, test_data)\u001b[0m\n\u001b[1;32m    563\u001b[0m                                     label_type=LABEL_TYPE.Permutation, gpu=self.gpu, device=self.device)\n\u001b[1;32m    564\u001b[0m             train_ndcg5 = ndcg_at_k(ranker=ranker, test_data=train_data, k=5,\n\u001b[0;32m--> 565\u001b[0;31m                                     label_type=LABEL_TYPE.Permutation, gpu=self.gpu, device=self.device)\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             test_ndcg1 = ndcg_at_k(ranker=ranker, test_data=test_data, k=1,\n",
      "\u001b[0;32m~/universalizing-weak-supervision/code/ptranking/ptranking/ltr_adhoc/eval/eval_utils.py\u001b[0m in \u001b[0;36mndcg_at_k\u001b[0;34m(pred, ranker, test_data, k, label_type, gpu, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mbatch_rele_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mbatch_rele_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ranking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_rele_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_rele_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/universalizing-weak-supervision/code/ptranking/ptranking/base/ranker.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, batch_ranking, train)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# evaluation mode for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mbatch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ranking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_ranking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/universalizing-weak-supervision/code/ptranking/ptranking/base/ranker.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_ranking)\u001b[0m\n\u001b[1;32m    234\u001b[0m                         \u001b[0;31m# because batchnorm1d dim fixed as dim 1...(TODO: fix it)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                         \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                         \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ws-cardinality/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ws-cardinality/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ws-cardinality/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2015\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m     )\n\u001b[1;32m   2018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    dataset= datasets_factory.create_dataset(data_conf)\n",
    "    dataset.create_samples()\n",
    "    \n",
    "    if l2r_training_conf['use_weak_labels']:\n",
    "        Y_tilde, thetas = get_weak_labels(dataset, weak_sup_conf, root_path=root_path)\n",
    "        r_utils = RankingUtils(data_conf['dimension'])\n",
    "        kt = r_utils.mean_kt_distance(Y_tilde,dataset.Y)\n",
    "        print('kt distance: ', kt)\n",
    "        dataset.set_Y_tilde(Y_tilde)\n",
    "    else:\n",
    "        kt = None\n",
    "    \n",
    "    ptwrapper = PtrankingWrapper(data_conf=data_conf, weak_sup_conf=weak_sup_conf,\n",
    "                                 l2r_training_conf=l2r_training_conf, result_path=conf['results_path'],\n",
    "                                 wl_kt_distance = kt)\n",
    "    X_train, X_test, Y_train, Y_test = dataset.get_train_test_torch(use_weak_labels=l2r_training_conf['use_weak_labels'])\n",
    "    ptwrapper.set_data(X_train=X_train, X_test=X_test,\n",
    "                      Y_train=Y_train, Y_test=Y_test)\n",
    "    model = ptwrapper.get_model()\n",
    "    result = ptwrapper.train_model(model, verbose=1)\n",
    "    \n",
    "    with open(os.path.join(f'results/seed_{seed}.pickle'), 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "        \n",
    "    print(seed, max(result['test_tau']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-pressure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-latino",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-trustee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-drinking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-possession",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-abortion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-cardinality",
   "language": "python",
   "name": "ws-cardinality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
