{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustained-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "root_path = '../../../' # path to project root\n",
    "sys.path.append('{}/code'.format(root_path))\n",
    "sys.path.append('{}/code/core'.format(root_path))\n",
    "sys.path.append('{}/code/datasets/'.format(root_path))\n",
    "sys.path.insert(0,'{}/code/ptranking'.format(root_path))\n",
    "\n",
    "from core.ranking_utils import *\n",
    "from core.mallows import *\n",
    "from core.ws_ranking import *\n",
    "from core.ws_real_workflow import * \n",
    "from datasets.imdb_tmdb_dataset import * \n",
    "from datasets.basic_clmn_dataset import * \n",
    "from core.labelling.feature_lf import *\n",
    "from ptranking_wrapper import PtrankingWrapper\n",
    "import datasets_factory \n",
    "import numpy as np \n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "southeast-bunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples...\n",
      "kt distance:  0.08959999999999999\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/imdb_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/imdb_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [858.9718], train tau 0.2724006772041321, test_tau 0.29839998483657837,train_ndcg@1 tensor([0.7520]), test_ndcg@1 tensor([0.7185])\n",
      "epoch 1, loss [799.5963], train tau 0.2528010606765747, test_tau 0.2856000065803528,train_ndcg@1 tensor([0.7722]), test_ndcg@1 tensor([0.7400])\n",
      "epoch 2, loss [777.464], train tau 0.24170121550559998, test_tau 0.28540003299713135,train_ndcg@1 tensor([0.7850]), test_ndcg@1 tensor([0.7375])\n",
      "epoch 3, loss [764.0457], train tau 0.23820114135742188, test_tau 0.28460001945495605,train_ndcg@1 tensor([0.7972]), test_ndcg@1 tensor([0.7430])\n",
      "epoch 4, loss [755.7026], train tau 0.23290064930915833, test_tau 0.28280001878738403,train_ndcg@1 tensor([0.8010]), test_ndcg@1 tensor([0.7355])\n",
      "epoch 5, loss [746.8282], train tau 0.22920066118240356, test_tau 0.2760000228881836,train_ndcg@1 tensor([0.8048]), test_ndcg@1 tensor([0.7475])\n",
      "epoch 6, loss [741.465], train tau 0.22850075364112854, test_tau 0.27820003032684326,train_ndcg@1 tensor([0.8023]), test_ndcg@1 tensor([0.7410])\n",
      "epoch 7, loss [733.3701], train tau 0.22500064969062805, test_tau 0.27720001339912415,train_ndcg@1 tensor([0.8070]), test_ndcg@1 tensor([0.7420])\n",
      "epoch 8, loss [728.2706], train tau 0.2190006971359253, test_tau 0.2789999842643738,train_ndcg@1 tensor([0.8100]), test_ndcg@1 tensor([0.7495])\n",
      "epoch 9, loss [722.424], train tau 0.2158006727695465, test_tau 0.27880001068115234,train_ndcg@1 tensor([0.8150]), test_ndcg@1 tensor([0.7385])\n",
      "The experiment result is saved in ../../../tmp/results/default/imdb_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.08959999999999999\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/imdb_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/imdb_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [856.8895], train tau 0.2669007480144501, test_tau 0.281000018119812,train_ndcg@1 tensor([0.7610]), test_ndcg@1 tensor([0.7395])\n",
      "epoch 1, loss [789.7186], train tau 0.24880081415176392, test_tau 0.2760000228881836,train_ndcg@1 tensor([0.7825]), test_ndcg@1 tensor([0.7365])\n",
      "epoch 2, loss [772.953], train tau 0.2455008327960968, test_tau 0.273000031709671,train_ndcg@1 tensor([0.7840]), test_ndcg@1 tensor([0.7390])\n",
      "epoch 3, loss [761.8955], train tau 0.23740097880363464, test_tau 0.2768000364303589,train_ndcg@1 tensor([0.7997]), test_ndcg@1 tensor([0.7460])\n",
      "epoch 4, loss [754.51776], train tau 0.2323012351989746, test_tau 0.27159997820854187,train_ndcg@1 tensor([0.8045]), test_ndcg@1 tensor([0.7555])\n",
      "epoch 5, loss [748.51953], train tau 0.23090124130249023, test_tau 0.27199995517730713,train_ndcg@1 tensor([0.8083]), test_ndcg@1 tensor([0.7580])\n",
      "epoch 6, loss [741.43475], train tau 0.22690123319625854, test_tau 0.26919999718666077,train_ndcg@1 tensor([0.8130]), test_ndcg@1 tensor([0.7675])\n",
      "epoch 7, loss [735.0131], train tau 0.22380098700523376, test_tau 0.2694000005722046,train_ndcg@1 tensor([0.8125]), test_ndcg@1 tensor([0.7590])\n",
      "epoch 8, loss [730.5163], train tau 0.22120097279548645, test_tau 0.2685999870300293,train_ndcg@1 tensor([0.8177]), test_ndcg@1 tensor([0.7585])\n",
      "epoch 9, loss [724.5147], train tau 0.21740108728408813, test_tau 0.26819998025894165,train_ndcg@1 tensor([0.8205]), test_ndcg@1 tensor([0.7585])\n",
      "The experiment result is saved in ../../../tmp/results/default/imdb_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.08959999999999999\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/imdb_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/imdb_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [855.22266], train tau 0.27340084314346313, test_tau 0.2947999835014343,train_ndcg@1 tensor([0.7462]), test_ndcg@1 tensor([0.7245])\n",
      "epoch 1, loss [797.1118], train tau 0.2555009722709656, test_tau 0.28600001335144043,train_ndcg@1 tensor([0.7690]), test_ndcg@1 tensor([0.7390])\n",
      "epoch 2, loss [779.70874], train tau 0.24730104207992554, test_tau 0.28299999237060547,train_ndcg@1 tensor([0.7883]), test_ndcg@1 tensor([0.7345])\n",
      "epoch 3, loss [768.9298], train tau 0.24210086464881897, test_tau 0.27720004320144653,train_ndcg@1 tensor([0.7920]), test_ndcg@1 tensor([0.7505])\n",
      "epoch 4, loss [759.99506], train tau 0.23640087246894836, test_tau 0.27740001678466797,train_ndcg@1 tensor([0.7952]), test_ndcg@1 tensor([0.7420])\n",
      "epoch 5, loss [753.2139], train tau 0.22890067100524902, test_tau 0.2754000127315521,train_ndcg@1 tensor([0.8050]), test_ndcg@1 tensor([0.7420])\n",
      "epoch 6, loss [746.15967], train tau 0.2272007167339325, test_tau 0.2742000222206116,train_ndcg@1 tensor([0.8050]), test_ndcg@1 tensor([0.7490])\n",
      "epoch 7, loss [741.7853], train tau 0.2214008867740631, test_tau 0.2738000154495239,train_ndcg@1 tensor([0.8145]), test_ndcg@1 tensor([0.7455])\n",
      "epoch 8, loss [736.83765], train tau 0.21850070357322693, test_tau 0.2754000425338745,train_ndcg@1 tensor([0.8210]), test_ndcg@1 tensor([0.7390])\n",
      "epoch 9, loss [731.4951], train tau 0.21680116653442383, test_tau 0.27619999647140503,train_ndcg@1 tensor([0.8215]), test_ndcg@1 tensor([0.7400])\n",
      "The experiment result is saved in ../../../tmp/results/default/imdb_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.08959999999999999\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/imdb_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/imdb_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [856.0859], train tau 0.26930075883865356, test_tau 0.2939999997615814,train_ndcg@1 tensor([0.7638]), test_ndcg@1 tensor([0.7370])\n",
      "epoch 1, loss [792.68866], train tau 0.2542009651660919, test_tau 0.2832000255584717,train_ndcg@1 tensor([0.7803]), test_ndcg@1 tensor([0.7455])\n",
      "epoch 2, loss [772.5368], train tau 0.2441011667251587, test_tau 0.28360000252723694,train_ndcg@1 tensor([0.7945]), test_ndcg@1 tensor([0.7410])\n",
      "epoch 3, loss [759.7561], train tau 0.23310071229934692, test_tau 0.28199997544288635,train_ndcg@1 tensor([0.8020]), test_ndcg@1 tensor([0.7420])\n",
      "epoch 4, loss [748.8375], train tau 0.2280007004737854, test_tau 0.2833999991416931,train_ndcg@1 tensor([0.8080]), test_ndcg@1 tensor([0.7500])\n",
      "epoch 5, loss [740.85675], train tau 0.2266007959842682, test_tau 0.2839999496936798,train_ndcg@1 tensor([0.8125]), test_ndcg@1 tensor([0.7490])\n",
      "epoch 6, loss [734.2049], train tau 0.2233009934425354, test_tau 0.2845999598503113,train_ndcg@1 tensor([0.8127]), test_ndcg@1 tensor([0.7395])\n",
      "epoch 7, loss [727.4926], train tau 0.221500962972641, test_tau 0.2829999625682831,train_ndcg@1 tensor([0.8150]), test_ndcg@1 tensor([0.7420])\n",
      "epoch 8, loss [722.2825], train tau 0.2180006206035614, test_tau 0.28199997544288635,train_ndcg@1 tensor([0.8170]), test_ndcg@1 tensor([0.7450])\n",
      "epoch 9, loss [717.6567], train tau 0.21800050139427185, test_tau 0.2831999659538269,train_ndcg@1 tensor([0.8185]), test_ndcg@1 tensor([0.7490])\n",
      "The experiment result is saved in ../../../tmp/results/default/imdb_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.08959999999999999\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/imdb_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/imdb_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [839.6211], train tau 0.26530081033706665, test_tau 0.27880001068115234,train_ndcg@1 tensor([0.7642]), test_ndcg@1 tensor([0.7470])\n",
      "epoch 1, loss [789.85815], train tau 0.25280141830444336, test_tau 0.2768000066280365,train_ndcg@1 tensor([0.7750]), test_ndcg@1 tensor([0.7425])\n",
      "epoch 2, loss [776.5177], train tau 0.24520108103752136, test_tau 0.27260005474090576,train_ndcg@1 tensor([0.7880]), test_ndcg@1 tensor([0.7490])\n",
      "epoch 3, loss [765.2525], train tau 0.24010103940963745, test_tau 0.26920002698898315,train_ndcg@1 tensor([0.7903]), test_ndcg@1 tensor([0.7550])\n",
      "epoch 4, loss [759.0871], train tau 0.23510098457336426, test_tau 0.2696000337600708,train_ndcg@1 tensor([0.7997]), test_ndcg@1 tensor([0.7440])\n",
      "epoch 5, loss [751.88745], train tau 0.23440083861351013, test_tau 0.2742000222206116,train_ndcg@1 tensor([0.7987]), test_ndcg@1 tensor([0.7365])\n",
      "epoch 6, loss [747.2774], train tau 0.23030075430870056, test_tau 0.27219998836517334,train_ndcg@1 tensor([0.7997]), test_ndcg@1 tensor([0.7335])\n",
      "epoch 7, loss [742.4803], train tau 0.228200763463974, test_tau 0.2704000473022461,train_ndcg@1 tensor([0.7960]), test_ndcg@1 tensor([0.7355])\n",
      "epoch 8, loss [738.1215], train tau 0.2265007197856903, test_tau 0.2696000039577484,train_ndcg@1 tensor([0.8018]), test_ndcg@1 tensor([0.7410])\n",
      "epoch 9, loss [734.0904], train tau 0.2232007086277008, test_tau 0.2738000154495239,train_ndcg@1 tensor([0.8085]), test_ndcg@1 tensor([0.7440])\n",
      "The experiment result is saved in ../../../tmp/results/default/imdb_rating/result_summary.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples...\n",
      "kt distance:  0.17213333333333336\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/tomato_user_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/tomato_user_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [893.4701], train tau 0.3040001392364502, test_tau 0.31720003485679626,train_ndcg@1 tensor([0.7247]), test_ndcg@1 tensor([0.7170])\n",
      "epoch 1, loss [849.5851], train tau 0.2856002449989319, test_tau 0.3166000247001648,train_ndcg@1 tensor([0.7477]), test_ndcg@1 tensor([0.7090])\n",
      "epoch 2, loss [831.57544], train tau 0.272800087928772, test_tau 0.3184000253677368,train_ndcg@1 tensor([0.7600]), test_ndcg@1 tensor([0.7030])\n",
      "epoch 3, loss [820.1632], train tau 0.26760047674179077, test_tau 0.3176000118255615,train_ndcg@1 tensor([0.7673]), test_ndcg@1 tensor([0.7070])\n",
      "epoch 4, loss [811.889], train tau 0.2669004201889038, test_tau 0.32360005378723145,train_ndcg@1 tensor([0.7667]), test_ndcg@1 tensor([0.7030])\n",
      "epoch 5, loss [804.3169], train tau 0.2613005042076111, test_tau 0.3222000300884247,train_ndcg@1 tensor([0.7757]), test_ndcg@1 tensor([0.7020])\n",
      "epoch 6, loss [797.37976], train tau 0.258800745010376, test_tau 0.3181999921798706,train_ndcg@1 tensor([0.7800]), test_ndcg@1 tensor([0.7110])\n",
      "epoch 7, loss [791.1387], train tau 0.250000536441803, test_tau 0.318600058555603,train_ndcg@1 tensor([0.7872]), test_ndcg@1 tensor([0.7115])\n",
      "epoch 8, loss [786.2239], train tau 0.2506008446216583, test_tau 0.3140000104904175,train_ndcg@1 tensor([0.7822]), test_ndcg@1 tensor([0.7125])\n",
      "epoch 9, loss [782.32715], train tau 0.24410086870193481, test_tau 0.3176000118255615,train_ndcg@1 tensor([0.7950]), test_ndcg@1 tensor([0.7165])\n",
      "The experiment result is saved in ../../../tmp/results/default/tomato_user_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.17213333333333336\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/tomato_user_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/tomato_user_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [902.8636], train tau 0.313400000333786, test_tau 0.33420002460479736,train_ndcg@1 tensor([0.7260]), test_ndcg@1 tensor([0.7115])\n",
      "epoch 1, loss [857.9884], train tau 0.303000271320343, test_tau 0.3274000287055969,train_ndcg@1 tensor([0.7362]), test_ndcg@1 tensor([0.7105])\n",
      "epoch 2, loss [845.2655], train tau 0.29540038108825684, test_tau 0.3256000578403473,train_ndcg@1 tensor([0.7458]), test_ndcg@1 tensor([0.7145])\n",
      "epoch 3, loss [837.9031], train tau 0.293400377035141, test_tau 0.3242000341415405,train_ndcg@1 tensor([0.7485]), test_ndcg@1 tensor([0.7160])\n",
      "epoch 4, loss [831.4701], train tau 0.28600046038627625, test_tau 0.32260003685951233,train_ndcg@1 tensor([0.7573]), test_ndcg@1 tensor([0.7105])\n",
      "epoch 5, loss [825.54626], train tau 0.28180021047592163, test_tau 0.3190000653266907,train_ndcg@1 tensor([0.7640]), test_ndcg@1 tensor([0.7190])\n",
      "epoch 6, loss [821.00366], train tau 0.2790004014968872, test_tau 0.3168000280857086,train_ndcg@1 tensor([0.7655]), test_ndcg@1 tensor([0.7280])\n",
      "epoch 7, loss [817.05493], train tau 0.27660033106803894, test_tau 0.32040005922317505,train_ndcg@1 tensor([0.7678]), test_ndcg@1 tensor([0.7190])\n",
      "epoch 8, loss [811.6749], train tau 0.27270030975341797, test_tau 0.3190000653266907,train_ndcg@1 tensor([0.7730]), test_ndcg@1 tensor([0.7185])\n",
      "epoch 9, loss [808.0568], train tau 0.27220046520233154, test_tau 0.320000022649765,train_ndcg@1 tensor([0.7755]), test_ndcg@1 tensor([0.7215])\n",
      "The experiment result is saved in ../../../tmp/results/default/tomato_user_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.17213333333333336\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/tomato_user_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/tomato_user_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [893.4982], train tau 0.308300256729126, test_tau 0.3214000165462494,train_ndcg@1 tensor([0.7253]), test_ndcg@1 tensor([0.7150])\n",
      "epoch 1, loss [850.22], train tau 0.28980058431625366, test_tau 0.32199999690055847,train_ndcg@1 tensor([0.7445]), test_ndcg@1 tensor([0.7160])\n",
      "epoch 2, loss [836.403], train tau 0.28610047698020935, test_tau 0.3141999840736389,train_ndcg@1 tensor([0.7500]), test_ndcg@1 tensor([0.7320])\n",
      "epoch 3, loss [826.87415], train tau 0.28140053153038025, test_tau 0.30800002813339233,train_ndcg@1 tensor([0.7605]), test_ndcg@1 tensor([0.7415])\n",
      "epoch 4, loss [819.2495], train tau 0.27700096368789673, test_tau 0.3078000247478485,train_ndcg@1 tensor([0.7650]), test_ndcg@1 tensor([0.7320])\n",
      "epoch 5, loss [813.2149], train tau 0.27030113339424133, test_tau 0.3052000105381012,train_ndcg@1 tensor([0.7720]), test_ndcg@1 tensor([0.7325])\n",
      "epoch 6, loss [807.6652], train tau 0.26240110397338867, test_tau 0.3128000497817993,train_ndcg@1 tensor([0.7788]), test_ndcg@1 tensor([0.7215])\n",
      "epoch 7, loss [799.9758], train tau 0.2580009400844574, test_tau 0.31280001997947693,train_ndcg@1 tensor([0.7807]), test_ndcg@1 tensor([0.7275])\n",
      "epoch 8, loss [794.5927], train tau 0.2525009512901306, test_tau 0.31620001792907715,train_ndcg@1 tensor([0.7885]), test_ndcg@1 tensor([0.7165])\n",
      "epoch 9, loss [788.8316], train tau 0.25090086460113525, test_tau 0.3148000240325928,train_ndcg@1 tensor([0.7872]), test_ndcg@1 tensor([0.7160])\n",
      "The experiment result is saved in ../../../tmp/results/default/tomato_user_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.17213333333333336\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/tomato_user_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/tomato_user_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [890.4863], train tau 0.3036000728607178, test_tau 0.326200008392334,train_ndcg@1 tensor([0.7337]), test_ndcg@1 tensor([0.6970])\n",
      "epoch 1, loss [847.50354], train tau 0.2893005609512329, test_tau 0.3190000355243683,train_ndcg@1 tensor([0.7487]), test_ndcg@1 tensor([0.7175])\n",
      "epoch 2, loss [833.10425], train tau 0.2804006338119507, test_tau 0.3134000301361084,train_ndcg@1 tensor([0.7487]), test_ndcg@1 tensor([0.7250])\n",
      "epoch 3, loss [823.2241], train tau 0.2745007276535034, test_tau 0.3094000220298767,train_ndcg@1 tensor([0.7563]), test_ndcg@1 tensor([0.7215])\n",
      "epoch 4, loss [815.1546], train tau 0.26910072565078735, test_tau 0.303600013256073,train_ndcg@1 tensor([0.7642]), test_ndcg@1 tensor([0.7290])\n",
      "epoch 5, loss [809.4755], train tau 0.2651008367538452, test_tau 0.3094000220298767,train_ndcg@1 tensor([0.7775]), test_ndcg@1 tensor([0.7185])\n",
      "epoch 6, loss [804.0269], train tau 0.2621006965637207, test_tau 0.30800002813339233,train_ndcg@1 tensor([0.7832]), test_ndcg@1 tensor([0.7215])\n",
      "epoch 7, loss [798.0672], train tau 0.26010072231292725, test_tau 0.30800002813339233,train_ndcg@1 tensor([0.7843]), test_ndcg@1 tensor([0.7205])\n",
      "epoch 8, loss [794.33936], train tau 0.25600099563598633, test_tau 0.30800002813339233,train_ndcg@1 tensor([0.7920]), test_ndcg@1 tensor([0.7275])\n",
      "epoch 9, loss [789.9697], train tau 0.2531009614467621, test_tau 0.3108000159263611,train_ndcg@1 tensor([0.7925]), test_ndcg@1 tensor([0.7280])\n",
      "The experiment result is saved in ../../../tmp/results/default/tomato_user_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.17213333333333336\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/tomato_user_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/tomato_user_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [890.7928], train tau 0.31209996342658997, test_tau 0.33379995822906494,train_ndcg@1 tensor([0.7100]), test_ndcg@1 tensor([0.7140])\n",
      "epoch 1, loss [854.4289], train tau 0.29650023579597473, test_tau 0.32600003480911255,train_ndcg@1 tensor([0.7372]), test_ndcg@1 tensor([0.7105])\n",
      "epoch 2, loss [836.8481], train tau 0.2871004343032837, test_tau 0.3194000720977783,train_ndcg@1 tensor([0.7465]), test_ndcg@1 tensor([0.7085])\n",
      "epoch 3, loss [825.0167], train tau 0.2772004008293152, test_tau 0.31859999895095825,train_ndcg@1 tensor([0.7585]), test_ndcg@1 tensor([0.7145])\n",
      "epoch 4, loss [816.82623], train tau 0.26960060000419617, test_tau 0.31700003147125244,train_ndcg@1 tensor([0.7685]), test_ndcg@1 tensor([0.7105])\n",
      "epoch 5, loss [809.3374], train tau 0.2667006552219391, test_tau 0.3181999921798706,train_ndcg@1 tensor([0.7650]), test_ndcg@1 tensor([0.7115])\n",
      "epoch 6, loss [804.2932], train tau 0.2611008286476135, test_tau 0.3203999996185303,train_ndcg@1 tensor([0.7725]), test_ndcg@1 tensor([0.7150])\n",
      "epoch 7, loss [796.9172], train tau 0.25750094652175903, test_tau 0.31800001859664917,train_ndcg@1 tensor([0.7795]), test_ndcg@1 tensor([0.7180])\n",
      "epoch 8, loss [791.9153], train tau 0.25230127573013306, test_tau 0.32019999623298645,train_ndcg@1 tensor([0.7837]), test_ndcg@1 tensor([0.7215])\n",
      "epoch 9, loss [784.8281], train tau 0.25280100107192993, test_tau 0.32339999079704285,train_ndcg@1 tensor([0.7818]), test_ndcg@1 tensor([0.7220])\n",
      "The experiment result is saved in ../../../tmp/results/default/tomato_user_rating/result_summary.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples...\n",
      "kt distance:  0.12086666666666668\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/mv_lens_avg_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/mv_lens_avg_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [870.55536], train tau 0.2915002703666687, test_tau 0.30640003085136414,train_ndcg@1 tensor([0.7567]), test_ndcg@1 tensor([0.7205])\n",
      "epoch 1, loss [829.2147], train tau 0.28160035610198975, test_tau 0.298799991607666,train_ndcg@1 tensor([0.7573]), test_ndcg@1 tensor([0.7210])\n",
      "epoch 2, loss [811.0652], train tau 0.2767001986503601, test_tau 0.3009999990463257,train_ndcg@1 tensor([0.7715]), test_ndcg@1 tensor([0.7245])\n",
      "epoch 3, loss [801.22736], train tau 0.27320045232772827, test_tau 0.3051999807357788,train_ndcg@1 tensor([0.7768]), test_ndcg@1 tensor([0.7180])\n",
      "epoch 4, loss [794.00934], train tau 0.26640039682388306, test_tau 0.3001999855041504,train_ndcg@1 tensor([0.7832]), test_ndcg@1 tensor([0.7235])\n",
      "epoch 5, loss [788.8658], train tau 0.2608005106449127, test_tau 0.29820001125335693,train_ndcg@1 tensor([0.7893]), test_ndcg@1 tensor([0.7195])\n",
      "epoch 6, loss [783.7336], train tau 0.2561008334159851, test_tau 0.30000001192092896,train_ndcg@1 tensor([0.7925]), test_ndcg@1 tensor([0.7210])\n",
      "epoch 7, loss [778.3099], train tau 0.25190049409866333, test_tau 0.3052000403404236,train_ndcg@1 tensor([0.8000]), test_ndcg@1 tensor([0.7255])\n",
      "epoch 8, loss [771.9534], train tau 0.2502005696296692, test_tau 0.3022000193595886,train_ndcg@1 tensor([0.7965]), test_ndcg@1 tensor([0.7200])\n",
      "epoch 9, loss [767.54865], train tau 0.24770069122314453, test_tau 0.3030000329017639,train_ndcg@1 tensor([0.7962]), test_ndcg@1 tensor([0.7215])\n",
      "The experiment result is saved in ../../../tmp/results/default/mv_lens_avg_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.12086666666666668\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/mv_lens_avg_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/mv_lens_avg_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [880.58636], train tau 0.29060032963752747, test_tau 0.3030000329017639,train_ndcg@1 tensor([0.7425]), test_ndcg@1 tensor([0.7305])\n",
      "epoch 1, loss [828.46375], train tau 0.27910059690475464, test_tau 0.3027999997138977,train_ndcg@1 tensor([0.7563]), test_ndcg@1 tensor([0.7425])\n",
      "epoch 2, loss [811.08325], train tau 0.2696005702018738, test_tau 0.3017999529838562,train_ndcg@1 tensor([0.7673]), test_ndcg@1 tensor([0.7335])\n",
      "epoch 3, loss [800.33673], train tau 0.26220056414604187, test_tau 0.30479997396469116,train_ndcg@1 tensor([0.7707]), test_ndcg@1 tensor([0.7295])\n",
      "epoch 4, loss [790.35767], train tau 0.2573007643222809, test_tau 0.3051999807357788,train_ndcg@1 tensor([0.7717]), test_ndcg@1 tensor([0.7210])\n",
      "epoch 5, loss [782.75366], train tau 0.24800097942352295, test_tau 0.3041999638080597,train_ndcg@1 tensor([0.7782]), test_ndcg@1 tensor([0.7190])\n",
      "epoch 6, loss [775.9177], train tau 0.2435009479522705, test_tau 0.3076000213623047,train_ndcg@1 tensor([0.7830]), test_ndcg@1 tensor([0.7190])\n",
      "epoch 7, loss [768.0319], train tau 0.23650100827217102, test_tau 0.30539998412132263,train_ndcg@1 tensor([0.7903]), test_ndcg@1 tensor([0.7175])\n",
      "epoch 8, loss [761.6877], train tau 0.23470088839530945, test_tau 0.309999942779541,train_ndcg@1 tensor([0.7935]), test_ndcg@1 tensor([0.7175])\n",
      "epoch 9, loss [758.6962], train tau 0.23250117897987366, test_tau 0.3123999834060669,train_ndcg@1 tensor([0.7985]), test_ndcg@1 tensor([0.7085])\n",
      "The experiment result is saved in ../../../tmp/results/default/mv_lens_avg_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.12086666666666668\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/mv_lens_avg_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/mv_lens_avg_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss [885.6145], train tau 0.28300023078918457, test_tau 0.29899996519088745,train_ndcg@1 tensor([0.7580]), test_ndcg@1 tensor([0.7260])\n",
      "epoch 1, loss [819.1794], train tau 0.2684008479118347, test_tau 0.28939998149871826,train_ndcg@1 tensor([0.7653]), test_ndcg@1 tensor([0.7435])\n",
      "epoch 2, loss [802.74585], train tau 0.2602007985115051, test_tau 0.2879999876022339,train_ndcg@1 tensor([0.7747]), test_ndcg@1 tensor([0.7480])\n",
      "epoch 3, loss [795.24414], train tau 0.25560057163238525, test_tau 0.29360002279281616,train_ndcg@1 tensor([0.7728]), test_ndcg@1 tensor([0.7385])\n",
      "epoch 4, loss [786.81067], train tau 0.2513008117675781, test_tau 0.29659998416900635,train_ndcg@1 tensor([0.7780]), test_ndcg@1 tensor([0.7415])\n",
      "epoch 5, loss [781.9218], train tau 0.2499009668827057, test_tau 0.29580003023147583,train_ndcg@1 tensor([0.7840]), test_ndcg@1 tensor([0.7375])\n",
      "epoch 6, loss [773.4536], train tau 0.24570104479789734, test_tau 0.29740002751350403,train_ndcg@1 tensor([0.7895]), test_ndcg@1 tensor([0.7330])\n",
      "epoch 7, loss [768.92993], train tau 0.24230092763900757, test_tau 0.2994000017642975,train_ndcg@1 tensor([0.7955]), test_ndcg@1 tensor([0.7225])\n",
      "epoch 8, loss [764.04877], train tau 0.24050107598304749, test_tau 0.29600000381469727,train_ndcg@1 tensor([0.7900]), test_ndcg@1 tensor([0.7255])\n",
      "epoch 9, loss [758.52026], train tau 0.2432006597518921, test_tau 0.29739999771118164,train_ndcg@1 tensor([0.7910]), test_ndcg@1 tensor([0.7240])\n",
      "The experiment result is saved in ../../../tmp/results/default/mv_lens_avg_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.12086666666666668\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/mv_lens_avg_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/mv_lens_avg_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [877.9468], train tau 0.30040037631988525, test_tau 0.2996000051498413,train_ndcg@1 tensor([0.7258]), test_ndcg@1 tensor([0.7185])\n",
      "epoch 1, loss [836.87756], train tau 0.2833004295825958, test_tau 0.2935999929904938,train_ndcg@1 tensor([0.7437]), test_ndcg@1 tensor([0.7265])\n",
      "epoch 2, loss [817.06885], train tau 0.27150046825408936, test_tau 0.2895999848842621,train_ndcg@1 tensor([0.7542]), test_ndcg@1 tensor([0.7355])\n",
      "epoch 3, loss [804.76624], train tau 0.2654004693031311, test_tau 0.28759998083114624,train_ndcg@1 tensor([0.7623]), test_ndcg@1 tensor([0.7320])\n",
      "epoch 4, loss [798.0111], train tau 0.2634006440639496, test_tau 0.28759998083114624,train_ndcg@1 tensor([0.7705]), test_ndcg@1 tensor([0.7340])\n",
      "epoch 5, loss [790.82153], train tau 0.258500874042511, test_tau 0.2945999503135681,train_ndcg@1 tensor([0.7713]), test_ndcg@1 tensor([0.7320])\n",
      "epoch 6, loss [783.3407], train tau 0.25230103731155396, test_tau 0.29339998960494995,train_ndcg@1 tensor([0.7780]), test_ndcg@1 tensor([0.7285])\n",
      "epoch 7, loss [778.22614], train tau 0.2523011565208435, test_tau 0.29739999771118164,train_ndcg@1 tensor([0.7845]), test_ndcg@1 tensor([0.7275])\n",
      "epoch 8, loss [772.4603], train tau 0.244401216506958, test_tau 0.29760003089904785,train_ndcg@1 tensor([0.7940]), test_ndcg@1 tensor([0.7265])\n",
      "epoch 9, loss [764.98425], train tau 0.24540123343467712, test_tau 0.298799991607666,train_ndcg@1 tensor([0.7933]), test_ndcg@1 tensor([0.7285])\n",
      "The experiment result is saved in ../../../tmp/results/default/mv_lens_avg_rating/result_summary.pkl\n",
      "Generate samples...\n",
      "kt distance:  0.12086666666666668\n",
      "use_weak_labels:True, we will use weak labels\n",
      "Training data shape, X_train.shape torch.Size([1000, 5, 16]) Y_train.shape torch.Size([1000, 5])\n",
      "set_and_load_data in LTREvaluator\n",
      "(1000, 5, 16) (1000, 5) (1000,)\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/mv_lens_avg_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "data_dict {'data_id': 'imdb_tmdb', 'dir_data': 'data/imdb-tmdb/processed/extended-imdb-default/mv_lens_avg_rating', 'min_docs': 10, 'min_rele': 1, 'scale_data': False, 'scaler_id': None, 'scaler_level': None, 'train_presort': True, 'validation_presort': True, 'test_presort': True, 'train_batch_size': 64, 'validation_batch_size': 1, 'test_batch_size': 1, 'unknown_as_zero': False, 'binary_rele': False, 'num_features': 16, 'has_comment': False, 'label_type': <LABEL_TYPE.Permutation: 2>, 'max_rele_level': None, 'fold_num': 1}\n",
      "Sequential(\n",
      "  (FeatureBN): BatchNorm1d(16, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (L_1): Linear(in_features=16, out_features=30, bias=True)\n",
      "  (BN_1): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_1): ReLU()\n",
      "  (DR_2): Dropout(p=0.01, inplace=False)\n",
      "  (L_2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (BN_2): BatchNorm1d(30, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
      "  (ACT_2): ReLU()\n",
      "  (L_3): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0, loss [881.9329], train tau 0.2990003824234009, test_tau 0.2946000099182129,train_ndcg@1 tensor([0.7315]), test_ndcg@1 tensor([0.7210])\n",
      "epoch 1, loss [828.8653], train tau 0.28460046648979187, test_tau 0.28780001401901245,train_ndcg@1 tensor([0.7483]), test_ndcg@1 tensor([0.7205])\n",
      "epoch 2, loss [813.3458], train tau 0.2744007706642151, test_tau 0.287600040435791,train_ndcg@1 tensor([0.7605]), test_ndcg@1 tensor([0.7355])\n",
      "epoch 3, loss [802.96106], train tau 0.26580092310905457, test_tau 0.29520002007484436,train_ndcg@1 tensor([0.7675]), test_ndcg@1 tensor([0.7365])\n",
      "epoch 4, loss [792.8082], train tau 0.26070114970207214, test_tau 0.29360002279281616,train_ndcg@1 tensor([0.7753]), test_ndcg@1 tensor([0.7415])\n",
      "epoch 5, loss [785.3614], train tau 0.2549007833003998, test_tau 0.2998000383377075,train_ndcg@1 tensor([0.7772]), test_ndcg@1 tensor([0.7315])\n",
      "epoch 6, loss [779.1559], train tau 0.2513008713722229, test_tau 0.30320003628730774,train_ndcg@1 tensor([0.7860]), test_ndcg@1 tensor([0.7300])\n",
      "epoch 7, loss [773.27783], train tau 0.2471010386943817, test_tau 0.3009999990463257,train_ndcg@1 tensor([0.7847]), test_ndcg@1 tensor([0.7355])\n",
      "epoch 8, loss [767.2766], train tau 0.24240121245384216, test_tau 0.30240002274513245,train_ndcg@1 tensor([0.7865]), test_ndcg@1 tensor([0.7310])\n",
      "epoch 9, loss [762.38763], train tau 0.24430137872695923, test_tau 0.30879998207092285,train_ndcg@1 tensor([0.7868]), test_ndcg@1 tensor([0.7165])\n",
      "The experiment result is saved in ../../../tmp/results/default/mv_lens_avg_rating/result_summary.pkl\n"
     ]
    }
   ],
   "source": [
    "for feature in ['imdb_rating', 'tomato_user_rating', 'mv_lens_avg_rating']:\n",
    "    for seed in range(5):\n",
    "        config_file_path = '{}/configs/extended-imdb-tmdb_ranking_experiment_three.yaml'.format(root_path)\n",
    "        with open(config_file_path,'r') as conf_file:\n",
    "            conf = yaml.full_load(conf_file)\n",
    "            conf['project_root'] = root_path \n",
    "            exp_name = feature\n",
    "\n",
    "            # fix configuration\n",
    "            conf['data_conf']['n_train'] = 1000\n",
    "            conf['data_conf']['n_test'] = 500\n",
    "            conf['data_conf']['processed_data_path'] = os.path.join(conf['data_conf']['processed_data_path'],\n",
    "                                                                    exp_name)\n",
    "\n",
    "            # l2r_training_conf\n",
    "            conf['l2r_training_conf']['use_weak_labels'] = True\n",
    "            conf['l2r_training_conf']['model_checkpoint'] = os.path.join(conf['l2r_training_conf']['model_checkpoint'],\n",
    "                                                                    exp_name)\n",
    "\n",
    "            # weak_sup_conf\n",
    "            conf['weak_sup_conf']['checkpoint_path'] = os.path.join(conf['weak_sup_conf']['checkpoint_path'],\n",
    "                                                                    exp_name)\n",
    "            # result_path\n",
    "            conf['results_path'] = os.path.join(conf['results_path'], exp_name)\n",
    "\n",
    "\n",
    "            data_conf = conf['data_conf']\n",
    "            weak_sup_conf = conf['weak_sup_conf'] # For partial ranking experiments, we should give\n",
    "            l2r_training_conf = conf['l2r_training_conf']\n",
    "            data_conf['project_root'] = root_path\n",
    "\n",
    "            dataset= datasets_factory.create_dataset(data_conf)\n",
    "            dataset.create_samples()\n",
    "\n",
    "#             Y_tilde = []\n",
    "#             d = conf['data_conf']['dimension']\n",
    "#             lf = FeatureRankingLF(feature, d=d, highest_first=True)\n",
    "#             for row in dataset.lst_feature_map:\n",
    "#                 Y_tilde.append(lf.apply(row))\n",
    "\n",
    "#             with open(feature+'.pickle', 'wb') as f:\n",
    "#                 pickle.dump(Y_tilde, f)\n",
    "\n",
    "            with open(feature+'.pickle', 'rb') as f:\n",
    "                Y_tilde = pickle.load(f)\n",
    "            \n",
    "            r_utils = RankingUtils(data_conf['dimension'])\n",
    "            kt = r_utils.mean_kt_distance(Y_tilde,dataset.Y)\n",
    "            print('kt distance: ', kt)\n",
    "            dataset.set_Y_tilde(Y_tilde)\n",
    "\n",
    "            ptwrapper = PtrankingWrapper(data_conf=data_conf, weak_sup_conf=weak_sup_conf,\n",
    "                                 l2r_training_conf=l2r_training_conf, result_path=conf['results_path'],\n",
    "                                 wl_kt_distance = kt)\n",
    "            X_train, X_test, Y_train, Y_test = dataset.get_train_test_torch(use_weak_labels=l2r_training_conf['use_weak_labels'])\n",
    "            ptwrapper.set_data(X_train=X_train, X_test=X_test,\n",
    "                              Y_train=Y_train, Y_test=Y_test)\n",
    "            model = ptwrapper.get_model()\n",
    "            result = ptwrapper.train_model(model, verbose=1)\n",
    "\n",
    "#             result_path = os.path.join('results', f\"{exp_name}_{seed}.pickle\")\n",
    "            result_path = os.path.join('results', f\"refactoring_test_{exp_name}+_{seed}.pickle\")\n",
    "            with open(result_path, 'wb') as f:\n",
    "                pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-starter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-cardinality",
   "language": "python",
   "name": "ws-cardinality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
